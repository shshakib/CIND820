{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "6bd3622f6f3f8bb6e8b4643a72dc4bf7a75a1467dea0f2ef918aabb1c737dfb1"
   }
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Setep 5: Machine Learning Model\n",
    "\n",
    "<br>\n",
    "Clean up any values left from any previous steps"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick clean up\n",
    "for name in dir():\n",
    "    if not name.startswith('_'): # and name not in ['mbti_FE','mbti_Dataset', 'full_Lem_CV', 'full_Lem_Ngram', 'full_Lem_tfidf']:\n",
    "        del globals()[name]"
   ]
  },
  {
   "source": [
    "Load Dataset and results from previous steps."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load information from prevous steps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "mbti_Dataset = pd.read_csv('mbti_Dataset.csv')\n",
    "#mbti_FE = pd.read_csv('mbti_FE.csv')\n",
    "\n",
    "#full_Lem_CV = sparse.load_npz('full_Lem_CV.npz')\n",
    "#full_Lem_Ngram = sparse.load_npz('full_Lem_Ngram.npz')\n",
    "#full_Lem_tfidf = sparse.load_npz('full_Lem_tfidf.npz')\n",
    "features_Dic = {'Count Vectorizer': sparse.load_npz('full_Lem_CV.npz'), \n",
    "                'TFIDF Vectorizer': sparse.load_npz('full_Lem_tfidf.npz')}\n",
    "\n",
    "#np.save('full_Lem_CV', full_Lem_CV.toarray())\n",
    "#np.save('f:/full_Lem_Ngram', full_Lem_Ngram.toarray())\n",
    "#np.save('full_Lem_tfidf', full_Lem_tfidf.toarray())"
   ]
  },
  {
   "source": [
    "## 5-1: Random Forest Model\n",
    "### 5-1-1: Explorering Random Forest with Holdout test set + grid-search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          Type            Method  Estimators  Max_Depth  Precision    Recall  \\\n",
       "0   Introverts  Count Vectorizer          10       10.0   0.761383  1.000000   \n",
       "1   Introverts  Count Vectorizer          10       20.0   0.768786  0.999249   \n",
       "2   Introverts  Count Vectorizer          10       30.0   0.758242  0.996958   \n",
       "3   Introverts  Count Vectorizer          10        NaN   0.778708  0.978212   \n",
       "4   Introverts  Count Vectorizer          50       10.0   0.764841  1.000000   \n",
       "5   Introverts  Count Vectorizer          50       20.0   0.769452  1.000000   \n",
       "6   Introverts  Count Vectorizer          50       30.0   0.774063  1.000000   \n",
       "7   Introverts  Count Vectorizer          50        NaN   0.774957  1.000000   \n",
       "8   Introverts  Count Vectorizer         100       10.0   0.776945  1.000000   \n",
       "9   Introverts  Count Vectorizer         100       20.0   0.763689  1.000000   \n",
       "10  Introverts  Count Vectorizer         100       30.0   0.761960  1.000000   \n",
       "11  Introverts  Count Vectorizer         100        NaN   0.769497  1.000000   \n",
       "12  Introverts  TFIDF Vectorizer          10       10.0   0.764265  1.000000   \n",
       "13  Introverts  TFIDF Vectorizer          10       20.0   0.792605  0.999272   \n",
       "14  Introverts  TFIDF Vectorizer          10       30.0   0.766377  0.996983   \n",
       "15  Introverts  TFIDF Vectorizer          10        NaN   0.788450  0.962166   \n",
       "16  Introverts  TFIDF Vectorizer          50       10.0   0.763112  1.000000   \n",
       "17  Introverts  TFIDF Vectorizer          50       20.0   0.779700  1.000000   \n",
       "18  Introverts  TFIDF Vectorizer          50       30.0   0.767589  1.000000   \n",
       "19  Introverts  TFIDF Vectorizer          50        NaN   0.770075  0.999250   \n",
       "20  Introverts  TFIDF Vectorizer         100       10.0   0.785591  1.000000   \n",
       "21  Introverts  TFIDF Vectorizer         100       20.0   0.774063  1.000000   \n",
       "22  Introverts  TFIDF Vectorizer         100       30.0   0.779700  1.000000   \n",
       "23  Introverts  TFIDF Vectorizer         100        NaN   0.778996  1.000000   \n",
       "24  Intuitives  Count Vectorizer          10       10.0   0.854755  1.000000   \n",
       "25  Intuitives  Count Vectorizer          10       20.0   0.865130  1.000000   \n",
       "26  Intuitives  Count Vectorizer          10       30.0   0.872622  1.000000   \n",
       "27  Intuitives  Count Vectorizer          10        NaN   0.872549  1.000000   \n",
       "28  Intuitives  Count Vectorizer          50       10.0   0.876657  1.000000   \n",
       "29  Intuitives  Count Vectorizer          50       20.0   0.865130  1.000000   \n",
       "30  Intuitives  Count Vectorizer          50       30.0   0.870893  1.000000   \n",
       "31  Intuitives  Count Vectorizer          50        NaN   0.861015  1.000000   \n",
       "32  Intuitives  Count Vectorizer         100       10.0   0.874928  1.000000   \n",
       "33  Intuitives  Count Vectorizer         100       20.0   0.862248  1.000000   \n",
       "34  Intuitives  Count Vectorizer         100       30.0   0.843228  1.000000   \n",
       "35  Intuitives  Count Vectorizer         100        NaN   0.863977  1.000000   \n",
       "36  Intuitives  TFIDF Vectorizer          10       10.0   0.861095  1.000000   \n",
       "37  Intuitives  TFIDF Vectorizer          10       20.0   0.873775  1.000000   \n",
       "38  Intuitives  TFIDF Vectorizer          10       30.0   0.843228  1.000000   \n",
       "39  Intuitives  TFIDF Vectorizer          10        NaN   0.854755  1.000000   \n",
       "40  Intuitives  TFIDF Vectorizer          50       10.0   0.857061  1.000000   \n",
       "41  Intuitives  TFIDF Vectorizer          50       20.0   0.857061  1.000000   \n",
       "42  Intuitives  TFIDF Vectorizer          50       30.0   0.850720  1.000000   \n",
       "43  Intuitives  TFIDF Vectorizer          50        NaN   0.857637  1.000000   \n",
       "44  Intuitives  TFIDF Vectorizer         100       10.0   0.859366  1.000000   \n",
       "45  Intuitives  TFIDF Vectorizer         100       20.0   0.859942  1.000000   \n",
       "46  Intuitives  TFIDF Vectorizer         100       30.0   0.863977  1.000000   \n",
       "47  Intuitives  TFIDF Vectorizer         100        NaN   0.857637  1.000000   \n",
       "48     Feelers  Count Vectorizer          10       10.0   0.618705  0.823404   \n",
       "49     Feelers  Count Vectorizer          10       20.0   0.610313  0.804905   \n",
       "50     Feelers  Count Vectorizer          10       30.0   0.643446  0.739726   \n",
       "51     Feelers  Count Vectorizer          10        NaN   0.620635  0.839957   \n",
       "52     Feelers  Count Vectorizer          50       10.0   0.643710  0.935649   \n",
       "53     Feelers  Count Vectorizer          50       20.0   0.635659  0.896175   \n",
       "54     Feelers  Count Vectorizer          50       30.0   0.721048  0.871297   \n",
       "55     Feelers  Count Vectorizer          50        NaN   0.654810  0.893054   \n",
       "56     Feelers  Count Vectorizer         100       10.0   0.609157  0.974522   \n",
       "57     Feelers  Count Vectorizer         100       20.0   0.667449  0.921166   \n",
       "58     Feelers  Count Vectorizer         100       30.0   0.661501  0.908175   \n",
       "59     Feelers  Count Vectorizer         100        NaN   0.691237  0.906552   \n",
       "60     Feelers  TFIDF Vectorizer          10       10.0   0.613885  0.844421   \n",
       "61     Feelers  TFIDF Vectorizer          10       20.0   0.670055  0.748967   \n",
       "62     Feelers  TFIDF Vectorizer          10       30.0   0.640107  0.764706   \n",
       "63     Feelers  TFIDF Vectorizer          10        NaN   0.606260  0.786325   \n",
       "64     Feelers  TFIDF Vectorizer          50       10.0   0.601509  0.960570   \n",
       "65     Feelers  TFIDF Vectorizer          50       20.0   0.654574  0.900217   \n",
       "66     Feelers  TFIDF Vectorizer          50       30.0   0.672665  0.859803   \n",
       "67     Feelers  TFIDF Vectorizer          50        NaN   0.650041  0.856681   \n",
       "68     Feelers  TFIDF Vectorizer         100       10.0   0.609459  0.962647   \n",
       "69     Feelers  TFIDF Vectorizer         100       20.0   0.693135  0.884900   \n",
       "70     Feelers  TFIDF Vectorizer         100       30.0   0.705426  0.868505   \n",
       "71     Feelers  TFIDF Vectorizer         100        NaN   0.685477  0.888172   \n",
       "72  Perceivers  Count Vectorizer          10       10.0   0.612233  0.980971   \n",
       "73  Perceivers  Count Vectorizer          10       20.0   0.613166  0.942197   \n",
       "74  Perceivers  Count Vectorizer          10       30.0   0.616000  0.892754   \n",
       "75  Perceivers  Count Vectorizer          10        NaN   0.654947  0.757089   \n",
       "76  Perceivers  Count Vectorizer          50       10.0   0.604035  1.000000   \n",
       "77  Perceivers  Count Vectorizer          50       20.0   0.591329  0.998049   \n",
       "78  Perceivers  Count Vectorizer          50       30.0   0.601299  0.989310   \n",
       "79  Perceivers  Count Vectorizer          50        NaN   0.644795  0.958724   \n",
       "80  Perceivers  Count Vectorizer         100       10.0   0.614986  1.000000   \n",
       "81  Perceivers  Count Vectorizer         100       20.0   0.607287  0.999049   \n",
       "82  Perceivers  Count Vectorizer         100       30.0   0.620268  0.997191   \n",
       "83  Perceivers  Count Vectorizer         100        NaN   0.621266  0.982059   \n",
       "84  Perceivers  TFIDF Vectorizer          10       10.0   0.620201  0.980392   \n",
       "85  Perceivers  TFIDF Vectorizer          10       20.0   0.625241  0.915174   \n",
       "86  Perceivers  TFIDF Vectorizer          10       30.0   0.633588  0.866224   \n",
       "87  Perceivers  TFIDF Vectorizer          10        NaN   0.642105  0.692526   \n",
       "88  Perceivers  TFIDF Vectorizer          50       10.0   0.604732  1.000000   \n",
       "89  Perceivers  TFIDF Vectorizer          50       20.0   0.601166  0.994214   \n",
       "90  Perceivers  TFIDF Vectorizer          50       30.0   0.628419  0.991557   \n",
       "91  Perceivers  TFIDF Vectorizer          50        NaN   0.638407  0.931584   \n",
       "92  Perceivers  TFIDF Vectorizer         100       10.0   0.621902  1.000000   \n",
       "93  Perceivers  TFIDF Vectorizer         100       20.0   0.617919  0.998133   \n",
       "94  Perceivers  TFIDF Vectorizer         100       30.0   0.606872  0.998084   \n",
       "95  Perceivers  TFIDF Vectorizer         100        NaN   0.628872  0.972222   \n",
       "\n",
       "    Accuracy  \n",
       "0      0.761  \n",
       "1      0.769  \n",
       "2      0.757  \n",
       "3      0.770  \n",
       "4      0.765  \n",
       "5      0.769  \n",
       "6      0.774  \n",
       "7      0.775  \n",
       "8      0.777  \n",
       "9      0.764  \n",
       "10     0.762  \n",
       "11     0.770  \n",
       "12     0.764  \n",
       "13     0.793  \n",
       "14     0.765  \n",
       "15     0.770  \n",
       "16     0.763  \n",
       "17     0.780  \n",
       "18     0.768  \n",
       "19     0.770  \n",
       "20     0.786  \n",
       "21     0.774  \n",
       "22     0.780  \n",
       "23     0.779  \n",
       "24     0.855  \n",
       "25     0.865  \n",
       "26     0.873  \n",
       "27     0.873  \n",
       "28     0.877  \n",
       "29     0.865  \n",
       "30     0.871  \n",
       "31     0.861  \n",
       "32     0.875  \n",
       "33     0.862  \n",
       "34     0.843  \n",
       "35     0.864  \n",
       "36     0.861  \n",
       "37     0.874  \n",
       "38     0.843  \n",
       "39     0.855  \n",
       "40     0.857  \n",
       "41     0.857  \n",
       "42     0.851  \n",
       "43     0.858  \n",
       "44     0.859  \n",
       "45     0.860  \n",
       "46     0.864  \n",
       "47     0.858  \n",
       "48     0.629  \n",
       "49     0.633  \n",
       "50     0.633  \n",
       "51     0.639  \n",
       "52     0.671  \n",
       "53     0.674  \n",
       "54     0.737  \n",
       "55     0.698  \n",
       "56     0.647  \n",
       "57     0.713  \n",
       "58     0.714  \n",
       "59     0.733  \n",
       "60     0.631  \n",
       "61     0.654  \n",
       "62     0.641  \n",
       "63     0.609  \n",
       "64     0.644  \n",
       "65     0.695  \n",
       "66     0.706  \n",
       "67     0.677  \n",
       "68     0.647  \n",
       "69     0.723  \n",
       "70     0.731  \n",
       "71     0.722  \n",
       "72     0.612  \n",
       "73     0.610  \n",
       "74     0.604  \n",
       "75     0.609  \n",
       "76     0.604  \n",
       "77     0.591  \n",
       "78     0.605  \n",
       "79     0.650  \n",
       "80     0.615  \n",
       "81     0.608  \n",
       "82     0.622  \n",
       "83     0.624  \n",
       "84     0.617  \n",
       "85     0.613  \n",
       "86     0.614  \n",
       "87     0.578  \n",
       "88     0.605  \n",
       "89     0.602  \n",
       "90     0.635  \n",
       "91     0.633  \n",
       "92     0.622  \n",
       "93     0.618  \n",
       "94     0.610  \n",
       "95     0.638  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Type</th>\n      <th>Method</th>\n      <th>Estimators</th>\n      <th>Max_Depth</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Introverts</td>\n      <td>Count Vectorizer</td>\n      <td>10</td>\n      <td>10.0</td>\n      <td>0.761383</td>\n      <td>1.000000</td>\n      <td>0.761</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Introverts</td>\n      <td>Count Vectorizer</td>\n      <td>10</td>\n      <td>20.0</td>\n      <td>0.768786</td>\n      <td>0.999249</td>\n      <td>0.769</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Introverts</td>\n      <td>Count Vectorizer</td>\n      <td>10</td>\n      <td>30.0</td>\n      <td>0.758242</td>\n      <td>0.996958</td>\n      <td>0.757</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Introverts</td>\n      <td>Count Vectorizer</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>0.778708</td>\n      <td>0.978212</td>\n      <td>0.770</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Introverts</td>\n      <td>Count Vectorizer</td>\n      <td>50</td>\n      <td>10.0</td>\n      <td>0.764841</td>\n      <td>1.000000</td>\n      <td>0.765</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Introverts</td>\n      <td>Count Vectorizer</td>\n      <td>50</td>\n      <td>20.0</td>\n      <td>0.769452</td>\n      <td>1.000000</td>\n      <td>0.769</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Introverts</td>\n      <td>Count Vectorizer</td>\n      <td>50</td>\n      <td>30.0</td>\n      <td>0.774063</td>\n      <td>1.000000</td>\n      <td>0.774</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Introverts</td>\n      <td>Count Vectorizer</td>\n      <td>50</td>\n      <td>NaN</td>\n      <td>0.774957</td>\n      <td>1.000000</td>\n      <td>0.775</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Introverts</td>\n      <td>Count Vectorizer</td>\n      <td>100</td>\n      <td>10.0</td>\n      <td>0.776945</td>\n      <td>1.000000</td>\n      <td>0.777</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Introverts</td>\n      <td>Count Vectorizer</td>\n      <td>100</td>\n      <td>20.0</td>\n      <td>0.763689</td>\n      <td>1.000000</td>\n      <td>0.764</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Introverts</td>\n      <td>Count Vectorizer</td>\n      <td>100</td>\n      <td>30.0</td>\n      <td>0.761960</td>\n      <td>1.000000</td>\n      <td>0.762</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Introverts</td>\n      <td>Count Vectorizer</td>\n      <td>100</td>\n      <td>NaN</td>\n      <td>0.769497</td>\n      <td>1.000000</td>\n      <td>0.770</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Introverts</td>\n      <td>TFIDF Vectorizer</td>\n      <td>10</td>\n      <td>10.0</td>\n      <td>0.764265</td>\n      <td>1.000000</td>\n      <td>0.764</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Introverts</td>\n      <td>TFIDF Vectorizer</td>\n      <td>10</td>\n      <td>20.0</td>\n      <td>0.792605</td>\n      <td>0.999272</td>\n      <td>0.793</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Introverts</td>\n      <td>TFIDF Vectorizer</td>\n      <td>10</td>\n      <td>30.0</td>\n      <td>0.766377</td>\n      <td>0.996983</td>\n      <td>0.765</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Introverts</td>\n      <td>TFIDF Vectorizer</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>0.788450</td>\n      <td>0.962166</td>\n      <td>0.770</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>Introverts</td>\n      <td>TFIDF Vectorizer</td>\n      <td>50</td>\n      <td>10.0</td>\n      <td>0.763112</td>\n      <td>1.000000</td>\n      <td>0.763</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Introverts</td>\n      <td>TFIDF Vectorizer</td>\n      <td>50</td>\n      <td>20.0</td>\n      <td>0.779700</td>\n      <td>1.000000</td>\n      <td>0.780</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>Introverts</td>\n      <td>TFIDF Vectorizer</td>\n      <td>50</td>\n      <td>30.0</td>\n      <td>0.767589</td>\n      <td>1.000000</td>\n      <td>0.768</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Introverts</td>\n      <td>TFIDF Vectorizer</td>\n      <td>50</td>\n      <td>NaN</td>\n      <td>0.770075</td>\n      <td>0.999250</td>\n      <td>0.770</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Introverts</td>\n      <td>TFIDF Vectorizer</td>\n      <td>100</td>\n      <td>10.0</td>\n      <td>0.785591</td>\n      <td>1.000000</td>\n      <td>0.786</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>Introverts</td>\n      <td>TFIDF Vectorizer</td>\n      <td>100</td>\n      <td>20.0</td>\n      <td>0.774063</td>\n      <td>1.000000</td>\n      <td>0.774</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>Introverts</td>\n      <td>TFIDF Vectorizer</td>\n      <td>100</td>\n      <td>30.0</td>\n      <td>0.779700</td>\n      <td>1.000000</td>\n      <td>0.780</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>Introverts</td>\n      <td>TFIDF Vectorizer</td>\n      <td>100</td>\n      <td>NaN</td>\n      <td>0.778996</td>\n      <td>1.000000</td>\n      <td>0.779</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>Intuitives</td>\n      <td>Count Vectorizer</td>\n      <td>10</td>\n      <td>10.0</td>\n      <td>0.854755</td>\n      <td>1.000000</td>\n      <td>0.855</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>Intuitives</td>\n      <td>Count Vectorizer</td>\n      <td>10</td>\n      <td>20.0</td>\n      <td>0.865130</td>\n      <td>1.000000</td>\n      <td>0.865</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>Intuitives</td>\n      <td>Count Vectorizer</td>\n      <td>10</td>\n      <td>30.0</td>\n      <td>0.872622</td>\n      <td>1.000000</td>\n      <td>0.873</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>Intuitives</td>\n      <td>Count Vectorizer</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>0.872549</td>\n      <td>1.000000</td>\n      <td>0.873</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>Intuitives</td>\n      <td>Count Vectorizer</td>\n      <td>50</td>\n      <td>10.0</td>\n      <td>0.876657</td>\n      <td>1.000000</td>\n      <td>0.877</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>Intuitives</td>\n      <td>Count Vectorizer</td>\n      <td>50</td>\n      <td>20.0</td>\n      <td>0.865130</td>\n      <td>1.000000</td>\n      <td>0.865</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>Intuitives</td>\n      <td>Count Vectorizer</td>\n      <td>50</td>\n      <td>30.0</td>\n      <td>0.870893</td>\n      <td>1.000000</td>\n      <td>0.871</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>Intuitives</td>\n      <td>Count Vectorizer</td>\n      <td>50</td>\n      <td>NaN</td>\n      <td>0.861015</td>\n      <td>1.000000</td>\n      <td>0.861</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>Intuitives</td>\n      <td>Count Vectorizer</td>\n      <td>100</td>\n      <td>10.0</td>\n      <td>0.874928</td>\n      <td>1.000000</td>\n      <td>0.875</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>Intuitives</td>\n      <td>Count Vectorizer</td>\n      <td>100</td>\n      <td>20.0</td>\n      <td>0.862248</td>\n      <td>1.000000</td>\n      <td>0.862</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>Intuitives</td>\n      <td>Count Vectorizer</td>\n      <td>100</td>\n      <td>30.0</td>\n      <td>0.843228</td>\n      <td>1.000000</td>\n      <td>0.843</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>Intuitives</td>\n      <td>Count Vectorizer</td>\n      <td>100</td>\n      <td>NaN</td>\n      <td>0.863977</td>\n      <td>1.000000</td>\n      <td>0.864</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>Intuitives</td>\n      <td>TFIDF Vectorizer</td>\n      <td>10</td>\n      <td>10.0</td>\n      <td>0.861095</td>\n      <td>1.000000</td>\n      <td>0.861</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>Intuitives</td>\n      <td>TFIDF Vectorizer</td>\n      <td>10</td>\n      <td>20.0</td>\n      <td>0.873775</td>\n      <td>1.000000</td>\n      <td>0.874</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>Intuitives</td>\n      <td>TFIDF Vectorizer</td>\n      <td>10</td>\n      <td>30.0</td>\n      <td>0.843228</td>\n      <td>1.000000</td>\n      <td>0.843</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>Intuitives</td>\n      <td>TFIDF Vectorizer</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>0.854755</td>\n      <td>1.000000</td>\n      <td>0.855</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>Intuitives</td>\n      <td>TFIDF Vectorizer</td>\n      <td>50</td>\n      <td>10.0</td>\n      <td>0.857061</td>\n      <td>1.000000</td>\n      <td>0.857</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>Intuitives</td>\n      <td>TFIDF Vectorizer</td>\n      <td>50</td>\n      <td>20.0</td>\n      <td>0.857061</td>\n      <td>1.000000</td>\n      <td>0.857</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>Intuitives</td>\n      <td>TFIDF Vectorizer</td>\n      <td>50</td>\n      <td>30.0</td>\n      <td>0.850720</td>\n      <td>1.000000</td>\n      <td>0.851</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>Intuitives</td>\n      <td>TFIDF Vectorizer</td>\n      <td>50</td>\n      <td>NaN</td>\n      <td>0.857637</td>\n      <td>1.000000</td>\n      <td>0.858</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>Intuitives</td>\n      <td>TFIDF Vectorizer</td>\n      <td>100</td>\n      <td>10.0</td>\n      <td>0.859366</td>\n      <td>1.000000</td>\n      <td>0.859</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>Intuitives</td>\n      <td>TFIDF Vectorizer</td>\n      <td>100</td>\n      <td>20.0</td>\n      <td>0.859942</td>\n      <td>1.000000</td>\n      <td>0.860</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>Intuitives</td>\n      <td>TFIDF Vectorizer</td>\n      <td>100</td>\n      <td>30.0</td>\n      <td>0.863977</td>\n      <td>1.000000</td>\n      <td>0.864</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>Intuitives</td>\n      <td>TFIDF Vectorizer</td>\n      <td>100</td>\n      <td>NaN</td>\n      <td>0.857637</td>\n      <td>1.000000</td>\n      <td>0.858</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>Feelers</td>\n      <td>Count Vectorizer</td>\n      <td>10</td>\n      <td>10.0</td>\n      <td>0.618705</td>\n      <td>0.823404</td>\n      <td>0.629</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>Feelers</td>\n      <td>Count Vectorizer</td>\n      <td>10</td>\n      <td>20.0</td>\n      <td>0.610313</td>\n      <td>0.804905</td>\n      <td>0.633</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>Feelers</td>\n      <td>Count Vectorizer</td>\n      <td>10</td>\n      <td>30.0</td>\n      <td>0.643446</td>\n      <td>0.739726</td>\n      <td>0.633</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>Feelers</td>\n      <td>Count Vectorizer</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>0.620635</td>\n      <td>0.839957</td>\n      <td>0.639</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>Feelers</td>\n      <td>Count Vectorizer</td>\n      <td>50</td>\n      <td>10.0</td>\n      <td>0.643710</td>\n      <td>0.935649</td>\n      <td>0.671</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>Feelers</td>\n      <td>Count Vectorizer</td>\n      <td>50</td>\n      <td>20.0</td>\n      <td>0.635659</td>\n      <td>0.896175</td>\n      <td>0.674</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>Feelers</td>\n      <td>Count Vectorizer</td>\n      <td>50</td>\n      <td>30.0</td>\n      <td>0.721048</td>\n      <td>0.871297</td>\n      <td>0.737</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>Feelers</td>\n      <td>Count Vectorizer</td>\n      <td>50</td>\n      <td>NaN</td>\n      <td>0.654810</td>\n      <td>0.893054</td>\n      <td>0.698</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>Feelers</td>\n      <td>Count Vectorizer</td>\n      <td>100</td>\n      <td>10.0</td>\n      <td>0.609157</td>\n      <td>0.974522</td>\n      <td>0.647</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>Feelers</td>\n      <td>Count Vectorizer</td>\n      <td>100</td>\n      <td>20.0</td>\n      <td>0.667449</td>\n      <td>0.921166</td>\n      <td>0.713</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>Feelers</td>\n      <td>Count Vectorizer</td>\n      <td>100</td>\n      <td>30.0</td>\n      <td>0.661501</td>\n      <td>0.908175</td>\n      <td>0.714</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>Feelers</td>\n      <td>Count Vectorizer</td>\n      <td>100</td>\n      <td>NaN</td>\n      <td>0.691237</td>\n      <td>0.906552</td>\n      <td>0.733</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>Feelers</td>\n      <td>TFIDF Vectorizer</td>\n      <td>10</td>\n      <td>10.0</td>\n      <td>0.613885</td>\n      <td>0.844421</td>\n      <td>0.631</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>Feelers</td>\n      <td>TFIDF Vectorizer</td>\n      <td>10</td>\n      <td>20.0</td>\n      <td>0.670055</td>\n      <td>0.748967</td>\n      <td>0.654</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>Feelers</td>\n      <td>TFIDF Vectorizer</td>\n      <td>10</td>\n      <td>30.0</td>\n      <td>0.640107</td>\n      <td>0.764706</td>\n      <td>0.641</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>Feelers</td>\n      <td>TFIDF Vectorizer</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>0.606260</td>\n      <td>0.786325</td>\n      <td>0.609</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>Feelers</td>\n      <td>TFIDF Vectorizer</td>\n      <td>50</td>\n      <td>10.0</td>\n      <td>0.601509</td>\n      <td>0.960570</td>\n      <td>0.644</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>Feelers</td>\n      <td>TFIDF Vectorizer</td>\n      <td>50</td>\n      <td>20.0</td>\n      <td>0.654574</td>\n      <td>0.900217</td>\n      <td>0.695</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>Feelers</td>\n      <td>TFIDF Vectorizer</td>\n      <td>50</td>\n      <td>30.0</td>\n      <td>0.672665</td>\n      <td>0.859803</td>\n      <td>0.706</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>Feelers</td>\n      <td>TFIDF Vectorizer</td>\n      <td>50</td>\n      <td>NaN</td>\n      <td>0.650041</td>\n      <td>0.856681</td>\n      <td>0.677</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>Feelers</td>\n      <td>TFIDF Vectorizer</td>\n      <td>100</td>\n      <td>10.0</td>\n      <td>0.609459</td>\n      <td>0.962647</td>\n      <td>0.647</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>Feelers</td>\n      <td>TFIDF Vectorizer</td>\n      <td>100</td>\n      <td>20.0</td>\n      <td>0.693135</td>\n      <td>0.884900</td>\n      <td>0.723</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>Feelers</td>\n      <td>TFIDF Vectorizer</td>\n      <td>100</td>\n      <td>30.0</td>\n      <td>0.705426</td>\n      <td>0.868505</td>\n      <td>0.731</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>Feelers</td>\n      <td>TFIDF Vectorizer</td>\n      <td>100</td>\n      <td>NaN</td>\n      <td>0.685477</td>\n      <td>0.888172</td>\n      <td>0.722</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>Perceivers</td>\n      <td>Count Vectorizer</td>\n      <td>10</td>\n      <td>10.0</td>\n      <td>0.612233</td>\n      <td>0.980971</td>\n      <td>0.612</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>Perceivers</td>\n      <td>Count Vectorizer</td>\n      <td>10</td>\n      <td>20.0</td>\n      <td>0.613166</td>\n      <td>0.942197</td>\n      <td>0.610</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>Perceivers</td>\n      <td>Count Vectorizer</td>\n      <td>10</td>\n      <td>30.0</td>\n      <td>0.616000</td>\n      <td>0.892754</td>\n      <td>0.604</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>Perceivers</td>\n      <td>Count Vectorizer</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>0.654947</td>\n      <td>0.757089</td>\n      <td>0.609</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>Perceivers</td>\n      <td>Count Vectorizer</td>\n      <td>50</td>\n      <td>10.0</td>\n      <td>0.604035</td>\n      <td>1.000000</td>\n      <td>0.604</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>Perceivers</td>\n      <td>Count Vectorizer</td>\n      <td>50</td>\n      <td>20.0</td>\n      <td>0.591329</td>\n      <td>0.998049</td>\n      <td>0.591</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>Perceivers</td>\n      <td>Count Vectorizer</td>\n      <td>50</td>\n      <td>30.0</td>\n      <td>0.601299</td>\n      <td>0.989310</td>\n      <td>0.605</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>Perceivers</td>\n      <td>Count Vectorizer</td>\n      <td>50</td>\n      <td>NaN</td>\n      <td>0.644795</td>\n      <td>0.958724</td>\n      <td>0.650</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>Perceivers</td>\n      <td>Count Vectorizer</td>\n      <td>100</td>\n      <td>10.0</td>\n      <td>0.614986</td>\n      <td>1.000000</td>\n      <td>0.615</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>Perceivers</td>\n      <td>Count Vectorizer</td>\n      <td>100</td>\n      <td>20.0</td>\n      <td>0.607287</td>\n      <td>0.999049</td>\n      <td>0.608</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>Perceivers</td>\n      <td>Count Vectorizer</td>\n      <td>100</td>\n      <td>30.0</td>\n      <td>0.620268</td>\n      <td>0.997191</td>\n      <td>0.622</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>Perceivers</td>\n      <td>Count Vectorizer</td>\n      <td>100</td>\n      <td>NaN</td>\n      <td>0.621266</td>\n      <td>0.982059</td>\n      <td>0.624</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>Perceivers</td>\n      <td>TFIDF Vectorizer</td>\n      <td>10</td>\n      <td>10.0</td>\n      <td>0.620201</td>\n      <td>0.980392</td>\n      <td>0.617</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>Perceivers</td>\n      <td>TFIDF Vectorizer</td>\n      <td>10</td>\n      <td>20.0</td>\n      <td>0.625241</td>\n      <td>0.915174</td>\n      <td>0.613</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>Perceivers</td>\n      <td>TFIDF Vectorizer</td>\n      <td>10</td>\n      <td>30.0</td>\n      <td>0.633588</td>\n      <td>0.866224</td>\n      <td>0.614</td>\n    </tr>\n    <tr>\n      <th>87</th>\n      <td>Perceivers</td>\n      <td>TFIDF Vectorizer</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>0.642105</td>\n      <td>0.692526</td>\n      <td>0.578</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>Perceivers</td>\n      <td>TFIDF Vectorizer</td>\n      <td>50</td>\n      <td>10.0</td>\n      <td>0.604732</td>\n      <td>1.000000</td>\n      <td>0.605</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>Perceivers</td>\n      <td>TFIDF Vectorizer</td>\n      <td>50</td>\n      <td>20.0</td>\n      <td>0.601166</td>\n      <td>0.994214</td>\n      <td>0.602</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>Perceivers</td>\n      <td>TFIDF Vectorizer</td>\n      <td>50</td>\n      <td>30.0</td>\n      <td>0.628419</td>\n      <td>0.991557</td>\n      <td>0.635</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>Perceivers</td>\n      <td>TFIDF Vectorizer</td>\n      <td>50</td>\n      <td>NaN</td>\n      <td>0.638407</td>\n      <td>0.931584</td>\n      <td>0.633</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>Perceivers</td>\n      <td>TFIDF Vectorizer</td>\n      <td>100</td>\n      <td>10.0</td>\n      <td>0.621902</td>\n      <td>1.000000</td>\n      <td>0.622</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>Perceivers</td>\n      <td>TFIDF Vectorizer</td>\n      <td>100</td>\n      <td>20.0</td>\n      <td>0.617919</td>\n      <td>0.998133</td>\n      <td>0.618</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>Perceivers</td>\n      <td>TFIDF Vectorizer</td>\n      <td>100</td>\n      <td>30.0</td>\n      <td>0.606872</td>\n      <td>0.998084</td>\n      <td>0.610</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>Perceivers</td>\n      <td>TFIDF Vectorizer</td>\n      <td>100</td>\n      <td>NaN</td>\n      <td>0.628872</td>\n      <td>0.972222</td>\n      <td>0.638</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pd.set_option('display.max_rows', None, 'display.max_colwidth', 500)\n",
    "\n",
    "def rfClassifier_GridSearch(X_Features, dataset_PD, test_Szie, Lable, feature_type, n_est, depth, is_print = True,):\n",
    "    if Lable == 'IE':\n",
    "        Lable = 'I'\n",
    "        predict_Lable = 'Introverts'\n",
    "    elif Lable == 'NS':\n",
    "        Lable = 'N'\n",
    "        predict_Lable = 'Intuitives'\n",
    "    elif Lable == 'FT':\n",
    "        Lable = 'F'\n",
    "        predict_Lable = 'Feelers'\n",
    "    elif Lable == 'PJ':\n",
    "        Lable = 'P'\n",
    "        predict_Lable = 'Perceivers'\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_Features, dataset_PD, test_size=test_Szie)#20% of dataset is test set\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_depth=depth ,n_jobs=-1)#Max depth of tree is 20\n",
    "    rf_model = rf.fit(X_train, Y_train)\n",
    "    Y_pred = rf_model.predict(X_test)\n",
    "    precision, recall, fscore, support = score(Y_test, Y_pred, pos_label=Lable, average='binary')\n",
    "    if is_print:\n",
    "        print('Being ' + predict_Lable + ' using ' + feature_type + ':'  ,\n",
    "        'Estimators: {} / Max_Depth: {} --> Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "                                                        n_est,\n",
    "                                                        depth,\n",
    "                                                        round(precision, 3),\n",
    "                                                        round(recall, 3), \n",
    "                                                        round((Y_pred==Y_test).sum() / len(Y_pred),3)))\n",
    "    return([predict_Lable, feature_type, n_est, depth, precision, recall, round((Y_pred==Y_test).sum() / len(Y_pred),3)])\n",
    "\n",
    "\n",
    "\n",
    "classes = ['IE' , 'NS', 'FT', 'PJ']\n",
    "rf_result = []\n",
    "\n",
    "for item in classes:\n",
    "    for key, X_Features in features_Dic.items():\n",
    "        for n_est in [10, 50, 100]:\n",
    "            for depth in [10, 20, 30, None]:\n",
    "                rf_result.append(rfClassifier_GridSearch(X_Features, mbti_Dataset[item], 0.2, item, key, n_est, depth, is_print=False))\n",
    "\n",
    "pd.DataFrame(rf_result, columns= ['Type', 'Method', 'Estimators', 'Max_Depth', 'Precision', 'Recall', 'Accuracy']).to_csv('RF_Holdout_Result.csv')\n",
    "pd.DataFrame(rf_result, columns= ['Type', 'Method', 'Estimators', 'Max_Depth', 'Precision', 'Recall', 'Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### 5-1-2: Evaluation Random Forest Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   param_max_depth param_n_estimators  std_test_score  mean_test_score  \\\n",
      "6               90                 10        0.004380         0.771873   \n",
      "0               30                 10        0.001094         0.770605   \n",
      "10            None                150        0.000565         0.770490   \n",
      "11            None                300        0.000461         0.770375   \n",
      "7               90                150        0.000461         0.770259   \n",
      "\n",
      "    rank_test_score  \n",
      "6                 1  \n",
      "0                 2  \n",
      "10                3  \n",
      "11                4  \n",
      "7                 5  \n",
      "Count Vectorizer\n",
      "   param_max_depth param_n_estimators  std_test_score  mean_test_score  \\\n",
      "10            None                150        0.000936         0.770259   \n",
      "8               90                300        0.000431         0.770144   \n",
      "11            None                300        0.000672         0.770144   \n",
      "4               60                150        0.000365         0.770029   \n",
      "5               60                300        0.000365         0.770029   \n",
      "\n",
      "    rank_test_score  \n",
      "10                1  \n",
      "8                 2  \n",
      "11                2  \n",
      "4                 4  \n",
      "5                 4  \n",
      "TFIDF Vectorizer\n",
      "  param_max_depth param_n_estimators  std_test_score  mean_test_score  \\\n",
      "6              90                 10        0.001314         0.862824   \n",
      "3              60                 10        0.000365         0.862248   \n",
      "0              30                 10        0.000282         0.862017   \n",
      "1              30                150        0.000282         0.862017   \n",
      "2              30                300        0.000282         0.862017   \n",
      "\n",
      "   rank_test_score  \n",
      "6                1  \n",
      "3                2  \n",
      "0                3  \n",
      "1                3  \n",
      "2                3  \n",
      "Count Vectorizer\n",
      "  param_max_depth param_n_estimators  std_test_score  mean_test_score  \\\n",
      "3              60                 10        0.000863         0.862478   \n",
      "9            None                 10        0.000565         0.862363   \n",
      "1              30                150        0.000282         0.862017   \n",
      "2              30                300        0.000282         0.862017   \n",
      "4              60                150        0.000282         0.862017   \n",
      "\n",
      "   rank_test_score  \n",
      "3                1  \n",
      "9                2  \n",
      "1                3  \n",
      "2                3  \n",
      "4                3  \n",
      "TFIDF Vectorizer\n",
      "   param_max_depth param_n_estimators  std_test_score  mean_test_score  \\\n",
      "5               60                300        0.005996         0.747896   \n",
      "8               90                300        0.007018         0.747896   \n",
      "11            None                300        0.008243         0.747666   \n",
      "7               90                150        0.006341         0.743862   \n",
      "2               30                300        0.005828         0.743170   \n",
      "\n",
      "    rank_test_score  \n",
      "5                 1  \n",
      "8                 1  \n",
      "11                3  \n",
      "7                 4  \n",
      "2                 5  \n",
      "Count Vectorizer\n",
      "   param_max_depth param_n_estimators  std_test_score  mean_test_score  \\\n",
      "8               90                300        0.008031         0.747666   \n",
      "11            None                300        0.014520         0.745591   \n",
      "5               60                300        0.007203         0.744553   \n",
      "4               60                150        0.010211         0.739712   \n",
      "2               30                300        0.007453         0.737752   \n",
      "\n",
      "    rank_test_score  \n",
      "8                 1  \n",
      "11                2  \n",
      "5                 3  \n",
      "4                 4  \n",
      "2                 5  \n",
      "TFIDF Vectorizer\n",
      "   param_max_depth param_n_estimators  std_test_score  mean_test_score  \\\n",
      "10            None                150        0.004739         0.621902   \n",
      "7               90                150        0.002323         0.618790   \n",
      "4               60                150        0.002536         0.617983   \n",
      "11            None                300        0.003102         0.617406   \n",
      "8               90                300        0.002856         0.616830   \n",
      "\n",
      "    rank_test_score  \n",
      "10                1  \n",
      "7                 2  \n",
      "4                 3  \n",
      "11                4  \n",
      "8                 5  \n",
      "Count Vectorizer\n",
      "   param_max_depth param_n_estimators  std_test_score  mean_test_score  \\\n",
      "10            None                150        0.003470         0.626282   \n",
      "7               90                150        0.003019         0.622709   \n",
      "4               60                150        0.003059         0.619481   \n",
      "8               90                300        0.002713         0.618329   \n",
      "11            None                300        0.004488         0.617061   \n",
      "\n",
      "    rank_test_score  \n",
      "10                1  \n",
      "7                 2  \n",
      "4                 3  \n",
      "8                 4  \n",
      "11                5  \n",
      "TFIDF Vectorizer\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "\n",
    "def RF_Evaluation(param, X_Features, dataset_PD, lable, is_print = True):\n",
    "    rf = RandomForestClassifier()\n",
    "    gs = GridSearchCV(rf, param, cv=5, n_jobs=-1)#cv=5 means 5 folde validation\n",
    "    gs_fit = gs.fit(X_Features, dataset_PD)\n",
    "    if is_print:\n",
    "        print(pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending = False)[['param_max_depth',\n",
    "        'param_n_estimators', 'std_test_score', 'mean_test_score', 'rank_test_score']][0:5])\n",
    "\n",
    "    return(pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending = False)[['param_max_depth',\n",
    "    'param_n_estimators', 'std_test_score', 'mean_test_score', 'rank_test_score']][0:5])\n",
    "\n",
    "\n",
    "param = {'n_estimators' : [10, 150, 300],\n",
    "        'max_depth' : [30, 60, 90, None]}\n",
    "\n",
    "classes = ['IE' , 'NS', 'FT', 'PJ']\n",
    "for item in classes:\n",
    "    for key, X_Features in features_Dic.items():\n",
    "        RF_Evaluation(param, X_Features, mbti_Dataset[item], item)\n",
    "        print(key)\n"
   ]
  },
  {
   "source": [
    "## 5-2: Gradient Boosting Model\n",
    "### 5-2-1: Gradient Boosting with Holdout test set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-dd506cc38405>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdepth\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.00\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m                     \u001b[0mgb_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGBoosting_GridSearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_Features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmbti_Dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_est\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_print\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Moving to {} as Max_Depth at: '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Moving to {} estimators at: '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_est\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-dd506cc38405>\u001b[0m in \u001b[0;36mGBoosting_GridSearch\u001b[1;34m(X_Features, dataset_PD, test_Szie, Lable, feature_type, n_est, depth, lr, is_print)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_Features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_PD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_Szie\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 20% of our dataset is test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_est\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#Max depth of tree is 20\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mgb_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mY_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgb_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msupport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         n_stages = self._fit_stages(\n\u001b[0m\u001b[0;32m    499\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m             \u001b[1;31m# fit next stage of trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[0;32m    556\u001b[0m                 \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m                 random_state, X_idx_sorted, X_csc, X_csr)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0m\u001b[0;32m    212\u001b[0m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1240\u001b[0m         \"\"\"\n\u001b[0;32m   1241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1242\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m   1243\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1244\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None, 'display.max_colwidth', 500)\n",
    "\n",
    "\n",
    "def GBoosting_GridSearch(X_Features, dataset_PD, test_Szie, Lable, feature_type , n_est, depth, lr ,is_print = True):\n",
    "    if Lable == 'IE':\n",
    "        Lable = 'I'\n",
    "        predict_Lable = 'Introverts'\n",
    "    elif Lable == 'NS':\n",
    "        Lable = 'N'\n",
    "        predict_Lable = 'Intuitives'\n",
    "    elif Lable == 'FT':\n",
    "        Lable = 'F'\n",
    "        predict_Lable = 'Feelers'\n",
    "    elif Lable == 'PJ':\n",
    "        Lable = 'P'\n",
    "        predict_Lable = 'Perceivers'\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_Features, dataset_PD, test_size=test_Szie) # 20% of our dataset is test set\n",
    "    gb = GradientBoostingClassifier(n_estimators=n_est, max_depth=depth , learning_rate=lr)#Max depth of tree is 20\n",
    "    gb_model = gb.fit(X_train, Y_train)\n",
    "    Y_pred = gb_model.predict(X_test)\n",
    "    precision, recall, fscore, support = score(Y_test, Y_pred, pos_label=Lable, average='binary')\n",
    "    if is_print:\n",
    "        print('Being ' + predict_Lable + ' using ' + feature_type + ': '  ,'Estimators: {} / Max_Depth: {} / Learning_Rate: {} --> Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "                                                        n_est,\n",
    "                                                        depth,\n",
    "                                                        lr,\n",
    "                                                        round(precision, 3),\n",
    "                                                        round(recall, 3), \n",
    "                                                        round((Y_pred==Y_test).sum() / len(Y_pred),3)))\n",
    "    return([predict_Lable, feature_type, n_est, depth, lr, precision, recall, round((Y_pred==Y_test).sum() / len(Y_pred),3)])\n",
    "\n",
    "\n",
    "\n",
    "classes = ['IE' , 'NS', 'FT', 'PJ']\n",
    "gb_result = []\n",
    "\n",
    "for item in classes:\n",
    "    for key, X_Features in features_Dic.items():\n",
    "        for n_est in [50, 100, 150]:\n",
    "            for depth in [3, 7, 11, 15]:\n",
    "                for lr in [0.01, 0.10, 1.00]:\n",
    "                    gb_result.append(GBoosting_GridSearch(X_Features, mbti_Dataset[item], 0.2, item, key, n_est, depth, lr, is_print=False))\n",
    "                print('Moving to {} as Max_Depth at: '.format(depth), datetime.now())\n",
    "            print('Moving to {} estimators at: '.format(n_est), datetime.now())\n",
    "        print('Moving to {} feature at: '.format(key), datetime.now())\n",
    "    print('Moving to {} class at: '.format(item), datetime.now())\n",
    "\n",
    "pd.DataFrame(gb_result, columns= ['Type', 'Method', 'Estimators', 'Max_Depth', 'Learning_Rate','Precision', 'Recall', 'Accuracy']).to_csv('GB_Holdout_Result.csv')\n",
    "pd.DataFrame(gb_result, columns= ['Type', 'Method', 'Estimators', 'Max_Depth', 'Learning_Rate','Precision', 'Recall', 'Accuracy']).head()\n"
   ]
  },
  {
   "source": [
    "### 5-2-2: Evaluation Gradient Boosting Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "\n",
    "\n",
    "def GB_Evaluation(param, X_Features, dataset_PD, lable):\n",
    "    gb = GradientBoostingClassifier()\n",
    "    gs = GridSearchCV(gb, param, cv=5)#cv=5 meand 5 folde validation\n",
    "    gs_fit = gs.fit(X_Features, dataset_PD)\n",
    "    print(pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending = False)[['param_max_depth',\n",
    "     'param_n_estimators', 'std_test_score', 'mean_test_score', 'rank_test_score']][0:5])\n",
    "\n",
    "    return(pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending = False)[['param_max_depth',\n",
    "     'param_n_estimators', 'std_test_score', 'mean_test_score', 'rank_test_score']][0:5])\n",
    "  \n",
    "param = {'n_estimators' : [50, 100, 150],\n",
    "                'max_depth' : [7, 11, 15],\n",
    "                'Learning_rate' : [0.1]}\n",
    "\n",
    "classes = ['IE' , 'NS', 'FT', 'PJ']\n",
    "\n",
    "\n",
    "for item in classes:\n",
    "    for key, X_Features in features_Dic.items():\n",
    "        GB_Evaluation(param, X_Features, mbti_Dataset[item], item)\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "frequency = 2500  # Set Frequency To 2500 Hertz\n",
    "duration = 1000  # Set Duration To 1000 ms == 1 second\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}