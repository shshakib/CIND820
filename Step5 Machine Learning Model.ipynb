{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "6bd3622f6f3f8bb6e8b4643a72dc4bf7a75a1467dea0f2ef918aabb1c737dfb1"
   }
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Setep 5: Machine Learning Model\n",
    "\n",
    "<br>\n",
    "Clean up any values left from any previous steps"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick clean up\n",
    "for name in dir():\n",
    "    if not name.startswith('_'): # and name not in ['mbti_FE','mbti_Dataset', 'full_Lem_CV', 'full_Lem_Ngram', 'full_Lem_tfidf']:\n",
    "        del globals()[name]"
   ]
  },
  {
   "source": [
    "Load Dataset and results from previous steps."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load information from prevous steps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "mbti_Dataset = pd.read_csv('mbti_Dataset.csv')\n",
    "mbti_FE = pd.read_csv('mbti_FE.csv')\n",
    "\n",
    "full_Lem_CV = sparse.load_npz('full_Lem_CV.npz')\n",
    "#full_Lem_Ngram = sparse.load_npz('full_Lem_Ngram.npz')\n",
    "full_Lem_tfidf = sparse.load_npz('full_Lem_tfidf.npz')\n",
    "features_Dic = {'Count Vectorizer': sparse.load_npz('full_Lem_CV.npz'), \n",
    "                'TFIDF Vectorizer': sparse.load_npz('full_Lem_tfidf.npz')}\n",
    "\n",
    "#np.save('full_Lem_CV', full_Lem_CV.toarray())\n",
    "#np.save('f:/full_Lem_Ngram', full_Lem_Ngram.toarray())\n",
    "#np.save('full_Lem_tfidf', full_Lem_tfidf.toarray())"
   ]
  },
  {
   "source": [
    "## 5-1: Random Forest Model\n",
    "### 5-1-1: Random Forest with Holdout test set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Results for predicting being Introverts using Count Vectorizer --- Precision: 0.769 / Recall: 1.0 / Accuracy: 0.769\n",
      "\n",
      "\n",
      "Results for predicting being Introverts using TFIDF Vectorizer --- Precision: 0.751 / Recall: 1.0 / Accuracy: 0.751\n",
      "\n",
      "\n",
      "Results for predicting being Intuitives using Count Vectorizer --- Precision: 0.871 / Recall: 1.0 / Accuracy: 0.871\n",
      "\n",
      "\n",
      "Results for predicting being Intuitives using TFIDF Vectorizer --- Precision: 0.868 / Recall: 1.0 / Accuracy: 0.868\n",
      "\n",
      "\n",
      "Results for predicting being Feelers using Count Vectorizer --- Precision: 0.688 / Recall: 0.876 / Accuracy: 0.707\n",
      "\n",
      "\n",
      "Results for predicting being Feelers using TFIDF Vectorizer --- Precision: 0.669 / Recall: 0.905 / Accuracy: 0.707\n",
      "\n",
      "\n",
      "Results for predicting being Perceivers using Count Vectorizer --- Precision: 0.601 / Recall: 0.998 / Accuracy: 0.603\n",
      "\n",
      "\n",
      "Results for predicting being Perceivers using TFIDF Vectorizer --- Precision: 0.608 / Recall: 0.99 / Accuracy: 0.607\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "#additonal_Features = ['No_Characters', 'No_Words', 'No_Char-Capital', 'No_Words-Capital', 'No_Punctuations', 'No_WordsInQuotes', 'No_Sentences', 'No_UniqueWords', 'No_Stopwords', 'Avg_WordLength', 'Avg_SentLength', 'UniqueWrd_vs_NoWrd', 'Stopwords_vs_NoWrd','Sentiment_Score']\n",
    "\n",
    "classes = ['IE' , 'NS', 'FT', 'PJ']\n",
    "\n",
    "def rfClassifier_HoldhoutSet(X_Features, dataset_PD, test_Szie, Lable, feature_type , is_print = True):\n",
    "    if Lable == 'IE':\n",
    "        Lable = 'I'\n",
    "        predict_Lable = 'Introverts'\n",
    "    elif Lable == 'NS':\n",
    "        Lable = 'N'\n",
    "        predict_Lable = 'Intuitives'\n",
    "    elif Lable == 'FT':\n",
    "        Lable = 'F'\n",
    "        predict_Lable = 'Feelers'\n",
    "    elif Lable == 'PJ':\n",
    "        Lable = 'P'\n",
    "        predict_Lable = 'Perceivers'\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_Features, dataset_PD, test_size=test_Szie)#20% of dataset for test set\n",
    "    rf = RandomForestClassifier(n_estimators=50, max_depth=20 ,n_jobs=-1)#Max depth of tree is 20\n",
    "    rf_model = rf.fit(X_train, Y_train)\n",
    "    Y_pred = rf_model.predict(X_test)\n",
    "    precision, recall, fscore, support = score(Y_test, Y_pred, pos_label=Lable, average='binary')\n",
    "    if is_print:\n",
    "        print('Results for predicting being ' + predict_Lable + ' using ' + feature_type,\n",
    "        '--- Precision: {} / Recall: {} / Accuracy: {}'.format(round(precision, 3),\n",
    "                                                        round(recall, 3), \n",
    "                                                        round((Y_pred==Y_test).sum() / len(Y_pred),3)))\n",
    "    return([precision, recall, fscore, support])\n",
    "\n",
    "\n",
    "for item in classes:\n",
    "    for key, X_Features in features_Dic.items():\n",
    "        X_Features = X_Features.toarray()\n",
    "        X_Features = pd.DataFrame(X_Features)\n",
    "        rfClassifier_HoldhoutSet(X_Features, mbti_Dataset[item], 0.2, item, key)\n",
    "        print('\\n')"
   ]
  },
  {
   "source": [
    "### 5-1-2: Explorering Random Forest with Holdout test set + grid-search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classes = ['IE' , 'NS', 'FT', 'PJ']\n",
    "\n",
    "\n",
    "def rfClassifier_GridSearch(X_Features, dataset_PD, test_Szie, Lable, feature_type, n_est, depth, is_print = True,):\n",
    "    if Lable == 'IE':\n",
    "        Lable = 'I'\n",
    "        predict_Lable = 'Introverts'\n",
    "    elif Lable == 'NS':\n",
    "        Lable = 'N'\n",
    "        predict_Lable = 'Intuitives'\n",
    "    elif Lable == 'FT':\n",
    "        Lable = 'F'\n",
    "        predict_Lable = 'Feelers'\n",
    "    elif Lable == 'PJ':\n",
    "        Lable = 'P'\n",
    "        predict_Lable = 'Perceivers'\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_Features, dataset_PD, test_size=test_Szie)#20% of dataset is test set\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_depth=depth ,n_jobs=-1)#Max depth of tree is 20\n",
    "    rf_model = rf.fit(X_train, Y_train)\n",
    "    Y_pred = rf_model.predict(X_test)\n",
    "    precision, recall, fscore, support = score(Y_test, Y_pred, pos_label=Lable, average='binary')\n",
    "    if is_print:\n",
    "        print('Being ' + predict_Lable + ' using ' + feature_type + ':'  ,\n",
    "        'Estimators: {} / Max_Depth: {} --> Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "                                                        n_est,\n",
    "                                                        depth,\n",
    "                                                        round(precision, 3),\n",
    "                                                        round(recall, 3), \n",
    "                                                        round((Y_pred==Y_test).sum() / len(Y_pred),3)))\n",
    "    return([predict_Lable, feature_type, n_est, depth, precision, recall, round((Y_pred==Y_test).sum() / len(Y_pred),3)])\n",
    "\n",
    "\n",
    "for item in classes:\n",
    "    for key, X_Features in features_Dic.items():\n",
    "        for n_est in [10, 50, 100]:\n",
    "            for depth in [10, 20, 30, None]:\n",
    "                rfClassifier_GridSearch(X_Features, mbti_Dataset[item], 0.2, item, key, n_est, depth)\n",
    "                print('\\n')\n"
   ]
  },
  {
   "source": [
    "### 5-1-3: Evaluation Random Forest Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   param_max_depth param_n_estimators  std_test_score  mean_test_score  \\\n",
      "6               90                 10        0.004380         0.771873   \n",
      "0               30                 10        0.001094         0.770605   \n",
      "10            None                150        0.000565         0.770490   \n",
      "11            None                300        0.000461         0.770375   \n",
      "7               90                150        0.000461         0.770259   \n",
      "\n",
      "    rank_test_score  \n",
      "6                 1  \n",
      "0                 2  \n",
      "10                3  \n",
      "11                4  \n",
      "7                 5  \n",
      "Count Vectorizer\n",
      "   param_max_depth param_n_estimators  std_test_score  mean_test_score  \\\n",
      "10            None                150        0.000936         0.770259   \n",
      "8               90                300        0.000431         0.770144   \n",
      "11            None                300        0.000672         0.770144   \n",
      "4               60                150        0.000365         0.770029   \n",
      "5               60                300        0.000365         0.770029   \n",
      "\n",
      "    rank_test_score  \n",
      "10                1  \n",
      "8                 2  \n",
      "11                2  \n",
      "4                 4  \n",
      "5                 4  \n",
      "TFIDF Vectorizer\n",
      "  param_max_depth param_n_estimators  std_test_score  mean_test_score  \\\n",
      "6              90                 10        0.001314         0.862824   \n",
      "3              60                 10        0.000365         0.862248   \n",
      "0              30                 10        0.000282         0.862017   \n",
      "1              30                150        0.000282         0.862017   \n",
      "2              30                300        0.000282         0.862017   \n",
      "\n",
      "   rank_test_score  \n",
      "6                1  \n",
      "3                2  \n",
      "0                3  \n",
      "1                3  \n",
      "2                3  \n",
      "Count Vectorizer\n",
      "  param_max_depth param_n_estimators  std_test_score  mean_test_score  \\\n",
      "3              60                 10        0.000863         0.862478   \n",
      "9            None                 10        0.000565         0.862363   \n",
      "1              30                150        0.000282         0.862017   \n",
      "2              30                300        0.000282         0.862017   \n",
      "4              60                150        0.000282         0.862017   \n",
      "\n",
      "   rank_test_score  \n",
      "3                1  \n",
      "9                2  \n",
      "1                3  \n",
      "2                3  \n",
      "4                3  \n",
      "TFIDF Vectorizer\n",
      "   param_max_depth param_n_estimators  std_test_score  mean_test_score  \\\n",
      "5               60                300        0.005996         0.747896   \n",
      "8               90                300        0.007018         0.747896   \n",
      "11            None                300        0.008243         0.747666   \n",
      "7               90                150        0.006341         0.743862   \n",
      "2               30                300        0.005828         0.743170   \n",
      "\n",
      "    rank_test_score  \n",
      "5                 1  \n",
      "8                 1  \n",
      "11                3  \n",
      "7                 4  \n",
      "2                 5  \n",
      "Count Vectorizer\n",
      "   param_max_depth param_n_estimators  std_test_score  mean_test_score  \\\n",
      "8               90                300        0.008031         0.747666   \n",
      "11            None                300        0.014520         0.745591   \n",
      "5               60                300        0.007203         0.744553   \n",
      "4               60                150        0.010211         0.739712   \n",
      "2               30                300        0.007453         0.737752   \n",
      "\n",
      "    rank_test_score  \n",
      "8                 1  \n",
      "11                2  \n",
      "5                 3  \n",
      "4                 4  \n",
      "2                 5  \n",
      "TFIDF Vectorizer\n",
      "   param_max_depth param_n_estimators  std_test_score  mean_test_score  \\\n",
      "10            None                150        0.004739         0.621902   \n",
      "7               90                150        0.002323         0.618790   \n",
      "4               60                150        0.002536         0.617983   \n",
      "11            None                300        0.003102         0.617406   \n",
      "8               90                300        0.002856         0.616830   \n",
      "\n",
      "    rank_test_score  \n",
      "10                1  \n",
      "7                 2  \n",
      "4                 3  \n",
      "11                4  \n",
      "8                 5  \n",
      "Count Vectorizer\n",
      "   param_max_depth param_n_estimators  std_test_score  mean_test_score  \\\n",
      "10            None                150        0.003470         0.626282   \n",
      "7               90                150        0.003019         0.622709   \n",
      "4               60                150        0.003059         0.619481   \n",
      "8               90                300        0.002713         0.618329   \n",
      "11            None                300        0.004488         0.617061   \n",
      "\n",
      "    rank_test_score  \n",
      "10                1  \n",
      "7                 2  \n",
      "4                 3  \n",
      "8                 4  \n",
      "11                5  \n",
      "TFIDF Vectorizer\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "\n",
    "def RF_Evaluation(param, X_Features, dataset_PD, lable, is_print = True):\n",
    "    rf = RandomForestClassifier()\n",
    "    gs = GridSearchCV(rf, param, cv=5, n_jobs=-1)#cv=5 means 5 folde validation\n",
    "    gs_fit = gs.fit(X_Features, dataset_PD)\n",
    "    if is_print:\n",
    "        print(pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending = False)[['param_max_depth',\n",
    "        'param_n_estimators', 'std_test_score', 'mean_test_score', 'rank_test_score']][0:5])\n",
    "\n",
    "    return(pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending = False)[['param_max_depth',\n",
    "    'param_n_estimators', 'std_test_score', 'mean_test_score', 'rank_test_score']][0:5])\n",
    "\n",
    "\n",
    "param = {'n_estimators' : [10, 150, 300],\n",
    "        'max_depth' : [30, 60, 90, None]}\n",
    "\n",
    "classes = ['IE' , 'NS', 'FT', 'PJ']\n",
    "for item in classes:\n",
    "    for key, X_Features in features_Dic.items():\n",
    "        RF_Evaluation(param, X_Features, mbti_Dataset[item], item)\n",
    "        print(key)\n"
   ]
  },
  {
   "source": [
    "## 5-2: Gradient Boosting Model\n",
    "### 5-2-1: Gradient Boosting with Holdout test set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Being Introverts using TFIDF Vectorizer:  Estimators: 50 / Max_Depth: 3 / Learning_Rate: 0.01 --> Precision: 0.776 / Recall: 1.0 / Accuracy: 0.776\n",
      "[['Introverts', 'TFIDF Vectorizer', 50, 3, 0.01, 0.7763688760806916, 1.0, 0.776]]\n",
      "Being Introverts using TFIDF Vectorizer:  Estimators: 50 / Max_Depth: 3 / Learning_Rate: 0.1 --> Precision: 0.82 / Recall: 0.96 / Accuracy: 0.809\n",
      "[['Introverts', 'TFIDF Vectorizer', 50, 3, 0.01, 0.7763688760806916, 1.0, 0.776], ['Introverts', 'TFIDF Vectorizer', 50, 3, 0.1, 0.8196509372979961, 0.9598788796366389, 0.809]]\n",
      "Being Introverts using TFIDF Vectorizer:  Estimators: 50 / Max_Depth: 3 / Learning_Rate: 1.0 --> Precision: 0.817 / Recall: 0.893 / Accuracy: 0.764\n",
      "[['Introverts', 'TFIDF Vectorizer', 50, 3, 0.01, 0.7763688760806916, 1.0, 0.776], ['Introverts', 'TFIDF Vectorizer', 50, 3, 0.1, 0.8196509372979961, 0.9598788796366389, 0.809], ['Introverts', 'TFIDF Vectorizer', 50, 3, 1.0, 0.8168724279835391, 0.8934733683420856, 0.764]]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-7d532be697b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdepth\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.00\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                 \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGBoosting_GridSearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_Features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmbti_Dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_est\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-7d532be697b8>\u001b[0m in \u001b[0;36mGBoosting_GridSearch\u001b[1;34m(X_Features, dataset_PD, test_Szie, Lable, feature_type, n_est, depth, lr, is_print)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_Features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_PD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_Szie\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 20% of our dataset is test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_est\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#Max depth of tree is 20\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mgb_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mY_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgb_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msupport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'binary'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         n_stages = self._fit_stages(\n\u001b[0m\u001b[0;32m    499\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m             \u001b[1;31m# fit next stage of trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[0;32m    556\u001b[0m                 \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m                 random_state, X_idx_sorted, X_csc, X_csr)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0m\u001b[0;32m    212\u001b[0m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1240\u001b[0m         \"\"\"\n\u001b[0;32m   1241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1242\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m   1243\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1244\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "def GBoosting_GridSearch(X_Features, dataset_PD, test_Szie, Lable, feature_type , n_est, depth, lr ,is_print = True):\n",
    "    if Lable == 'IE':\n",
    "        Lable = 'I'\n",
    "        predict_Lable = 'Introverts'\n",
    "    elif Lable == 'NS':\n",
    "        Lable = 'N'\n",
    "        predict_Lable = 'Intuitives'\n",
    "    elif Lable == 'FT':\n",
    "        Lable = 'F'\n",
    "        predict_Lable = 'Feelers'\n",
    "    elif Lable == 'PJ':\n",
    "        Lable = 'P'\n",
    "        predict_Lable = 'Perceivers'\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_Features, dataset_PD, test_size=test_Szie) # 20% of our dataset is test set\n",
    "    gb = GradientBoostingClassifier(n_estimators=n_est, max_depth=depth , learning_rate=lr)#Max depth of tree is 20\n",
    "    gb_model = gb.fit(X_train, Y_train)\n",
    "    Y_pred = gb_model.predict(X_test)\n",
    "    precision, recall, fscore, support = score(Y_test, Y_pred, pos_label=Lable, average='binary')\n",
    "    if is_print:\n",
    "        print('Being ' + predict_Lable + ' using ' + feature_type + ': '  ,'Estimators: {} / Max_Depth: {} / Learning_Rate: {} --> Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "                                                        n_est,\n",
    "                                                        depth,\n",
    "                                                        lr,\n",
    "                                                        round(precision, 3),\n",
    "                                                        round(recall, 3), \n",
    "                                                        round((Y_pred==Y_test).sum() / len(Y_pred),3)))\n",
    "    return([predict_Lable, feature_type, n_est, depth, lr, precision, recall, round((Y_pred==Y_test).sum() / len(Y_pred),3)])\n",
    "\n",
    "\n",
    "\n",
    "classes = ['IE' , 'NS', 'FT', 'PJ']\n",
    "#result_NP = np.empty((2, 36))\n",
    "result = []\n",
    "i = 0\n",
    "for item in classes:\n",
    "    for n_est in [50, 100, 150]:\n",
    "        for depth in [3, 7, 11, 15]:\n",
    "            for lr in [0.01, 0.10, 1.00]:\n",
    "                result.append(GBoosting_GridSearch(X_Features, mbti_Dataset[item], 0.2, item, key, n_est, depth, lr))\n",
    "                #print(result)\n"
   ]
  },
  {
   "source": [
    "### 5-2-2: Evaluation Gradient Boosting Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "\n",
    "\n",
    "def GB_Evaluation(param, X_Features, dataset_PD, lable):\n",
    "    gb = GradientBoostingClassifier()\n",
    "    gs = GridSearchCV(gb, param, cv=5)#cv=5 meand 5 folde validation\n",
    "    gs_fit = gs.fit(X_Features, dataset_PD)\n",
    "    print(pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending = False)[['param_max_depth',\n",
    "     'param_n_estimators', 'std_test_score', 'mean_test_score', 'rank_test_score']][0:5])\n",
    "\n",
    "    return(pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending = False)[['param_max_depth',\n",
    "     'param_n_estimators', 'std_test_score', 'mean_test_score', 'rank_test_score']][0:5])\n",
    "  \n",
    "param = {'n_estimators' : [50, 100, 150],\n",
    "                'max_depth' : [7, 11, 15],\n",
    "                'Learning_rate' : [0.1]}\n",
    "\n",
    "classes = ['IE' , 'NS', 'FT', 'PJ']\n",
    "\n",
    "\n",
    "for item in classes:\n",
    "    for key, X_Features in features_Dic.items():\n",
    "        GB_Evaluation(param, X_Features, mbti_Dataset[item], item)\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "frequency = 2500  # Set Frequency To 2500 Hertz\n",
    "duration = 1000  # Set Duration To 1000 ms == 1 second\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}