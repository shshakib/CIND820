{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "6bd3622f6f3f8bb6e8b4643a72dc4bf7a75a1467dea0f2ef918aabb1c737dfb1"
   }
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Setep 5: Machine Learning Model\n",
    "\n",
    "<br>\n",
    "Clean up any values left from any previous steps"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick clean up\n",
    "def cleanup():\n",
    "    for name in dir():\n",
    "        if not name.startswith('_'):\n",
    "            del globals()[name]\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Type IE NS FT PJ  \\\n",
       "0  INFJ  I  N  F  J   \n",
       "1  ENTP  E  N  T  P   \n",
       "2  INTP  I  N  T  P   \n",
       "3  INTJ  I  N  T  J   \n",
       "4  ENTJ  E  N  T  J   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Posts_Lemmatized  \n",
       "0  [, moment, sportscenter, top, ten, play, prank, lifechanging, experience, life, repeat, today, may, perc, experience, immerse, last, thing, friend, posted, facebook, committing, suicide, next, day, rest, peace, hello, sorry, hear, distress, natural, relationship, perfection, time, every, moment, existence, try, figure, hard, time, time, growth, welcome, stuff, game, set, match, prozac, wellbrutin, least, thirty, minute, moving, leg, mean, moving, sitting, desk, chair, weed, moderation, maybe...  \n",
       "1  [finding, lack, post, alarming, sex, boring, position, often, example, girlfriend, currently, environment, creatively, use, cowgirl, missionary, enough, giving, new, meaning, game, theory, hello, grin, take, converse, flirting, acknowledge, presence, return, word, smooth, wordplay, cheeky, grin, lack, balance, hand, eye, coordination, real, iq, test, score, internet, iq, test, funny, score, higher, like, former, response, thread, mention, believe, iq, test, banish, know, vanish, site, year, ...  \n",
       "2  [good, one, course, say, know, blessing, curse, absolutely, positive, best, friend, could, amazing, couple, count, yes, could, madly, love, case, reconciled, feeling, thank, link, socalled, tisi, loop, stem, current, topicobsession, deadly, like, stuck, thought, mind, wanders, circle, feel, truly, terrible, noticed, peculiar, vegetation, look, grass, dozen, different, plant, specie, imagine, hundred, year, later, whenif, soil, smith, â, never, one, ever, often, find, spotting, face, marble, ...  \n",
       "3  [dear, enjoyed, conversation, day, esoteric, gabbing, nature, universe, idea, every, rule, social, code, arbitrary, construct, created, dear, sub, long, time, see, sincerely, alpha, none, type, hurt, deep, existential, way, want, part, probably, sliding, scale, depends, individual, preference, like, everything, humanity, draco, malfoy, also, would, say, either, either, though, stacking, somewhat, arbitrary, distinction, make, believe, core, indicates, primary, motivation, hand, every, action...  \n",
       "4  [fired, another, silly, misconception, approaching, logically, going, key, unlocking, whatever, think, entitled, nobody, want, approached, b, guy, really, want, go, superduperlongass, vacation, cmon, guy, bos, listen, get, even, approached, logically, everything, never, mind, go, permanent, vacation, two, month, would, crazy, idea, really, best, employee, may, cooking, want, reliable, asset, gone, long, employer, lol, like, view, unsolicited, victim, sometimes, really, like, impoverished, ra...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Type</th>\n      <th>IE</th>\n      <th>NS</th>\n      <th>FT</th>\n      <th>PJ</th>\n      <th>Posts_Lemmatized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>INFJ</td>\n      <td>I</td>\n      <td>N</td>\n      <td>F</td>\n      <td>J</td>\n      <td>[, moment, sportscenter, top, ten, play, prank, lifechanging, experience, life, repeat, today, may, perc, experience, immerse, last, thing, friend, posted, facebook, committing, suicide, next, day, rest, peace, hello, sorry, hear, distress, natural, relationship, perfection, time, every, moment, existence, try, figure, hard, time, time, growth, welcome, stuff, game, set, match, prozac, wellbrutin, least, thirty, minute, moving, leg, mean, moving, sitting, desk, chair, weed, moderation, maybe...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ENTP</td>\n      <td>E</td>\n      <td>N</td>\n      <td>T</td>\n      <td>P</td>\n      <td>[finding, lack, post, alarming, sex, boring, position, often, example, girlfriend, currently, environment, creatively, use, cowgirl, missionary, enough, giving, new, meaning, game, theory, hello, grin, take, converse, flirting, acknowledge, presence, return, word, smooth, wordplay, cheeky, grin, lack, balance, hand, eye, coordination, real, iq, test, score, internet, iq, test, funny, score, higher, like, former, response, thread, mention, believe, iq, test, banish, know, vanish, site, year, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>INTP</td>\n      <td>I</td>\n      <td>N</td>\n      <td>T</td>\n      <td>P</td>\n      <td>[good, one, course, say, know, blessing, curse, absolutely, positive, best, friend, could, amazing, couple, count, yes, could, madly, love, case, reconciled, feeling, thank, link, socalled, tisi, loop, stem, current, topicobsession, deadly, like, stuck, thought, mind, wanders, circle, feel, truly, terrible, noticed, peculiar, vegetation, look, grass, dozen, different, plant, specie, imagine, hundred, year, later, whenif, soil, smith, â, never, one, ever, often, find, spotting, face, marble, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>INTJ</td>\n      <td>I</td>\n      <td>N</td>\n      <td>T</td>\n      <td>J</td>\n      <td>[dear, enjoyed, conversation, day, esoteric, gabbing, nature, universe, idea, every, rule, social, code, arbitrary, construct, created, dear, sub, long, time, see, sincerely, alpha, none, type, hurt, deep, existential, way, want, part, probably, sliding, scale, depends, individual, preference, like, everything, humanity, draco, malfoy, also, would, say, either, either, though, stacking, somewhat, arbitrary, distinction, make, believe, core, indicates, primary, motivation, hand, every, action...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ENTJ</td>\n      <td>E</td>\n      <td>N</td>\n      <td>T</td>\n      <td>J</td>\n      <td>[fired, another, silly, misconception, approaching, logically, going, key, unlocking, whatever, think, entitled, nobody, want, approached, b, guy, really, want, go, superduperlongass, vacation, cmon, guy, bos, listen, get, even, approached, logically, everything, never, mind, go, permanent, vacation, two, month, would, crazy, idea, really, best, employee, may, cooking, want, reliable, asset, gone, long, employer, lol, like, view, unsolicited, victim, sometimes, really, like, impoverished, ra...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "#Load information from prevous steps\n",
    "import pandas as pd\n",
    "\n",
    "fields = ['Type', 'IE', 'NS', 'FT', 'PJ', 'Posts_Lemmatized']\n",
    "mbti_Dataset = pd.read_pickle('mbti_Dataset2.pkl')\n",
    "mbti_Dataset = mbti_Dataset.filter(fields)\n",
    "mbti_Dataset.head()"
   ]
  },
  {
   "source": [
    "## 5-1: Split our data into training and test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(mbti_Dataset['Posts_Lemmatized'], mbti_Dataset[['IE' , 'NS', 'FT', 'PJ']], test_size=0.2)#20% of dataset is test set    \n",
    "\n",
    "\n",
    "X_train.to_pickle('X_train')\n",
    "X_train.to_pickle('X_test')\n",
    "X_train.to_pickle('Y_train')\n",
    "X_train.to_pickle('Y_test')\n",
    "#np.save('Y_train', Y_train)#Saving results\n",
    "#np.save('Y_test', Y_test)#Saving results\n",
    "#np.save('X_train', Y_train)#Saving results\n",
    "#np.save('X_test', Y_test)#Saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "def lemmatizing(text):\n",
    "    \"\"\"Lemmatizing the input text using WordNet and NLTK package\"\"\"\n",
    "    NLTK_WNL = nltk.WordNetLemmatizer()\n",
    "    text_Lem = [NLTK_WNL.lemmatize(word) for word in text]\n",
    "    return(text_Lem)\n",
    "\n",
    "\n",
    "def clean_and_load():\n",
    "    for name in dir():\n",
    "        if not name.startswith('_'):\n",
    "            del globals()[name]\n",
    "    global X_train, X_test, Y_train, Y_test\n",
    "    X_train = pd.read_pickle('X_train')\n",
    "    X_test = pd.read_pickle('X_test')\n",
    "    Y_train = pd.read_pickle('Y_train')\n",
    "    Y_test = pd.read_pickle('Y_test')\n",
    "\n",
    "\n",
    "##TFIDF\n",
    "#tfidf_vect = TfidfVectorizer(analyzer=lemmatizing)\n",
    "#tfidf_vect_fit = tfidf_vect.fit(X_train)\n",
    "#\n",
    "#tfidf_train = tfidf_vect_fit.transform(X_train)\n",
    "#tfidf_test = tfidf_vect_fit.transform(X_test)\n",
    "#\n",
    "#X_train_tfidf = pd.DataFrame(tfidf_train.toarray())\n",
    "#X_test_tfidf = pd.DataFrame(tfidf_test.toarray())\n",
    "#\n",
    "#np.save('X_train_tfidf', X_train_tfidf)#Saving results\n",
    "#np.save('X_test_tfidf', X_test_tfidf)#Saving results\n",
    "\n",
    "\n",
    "\n",
    "#CountVectorizer\n",
    "#count_vect = CountVectorizer(analyzer=lemmatizing)\n",
    "#count_vect_fit = count_vect.fit(X_train)\n",
    "#\n",
    "#count_train = count_vect_fit.transform(X_train)\n",
    "#count_test = count_vect_fit.transform(X_test)\n",
    "#\n",
    "#X_train_count = pd.DataFrame(count_train.toarray())\n",
    "#X_test_count = pd.DataFrame(count_test.toarray())\n",
    "#\n",
    "#np.save('X_train_count', X_train_count)#Saving results\n",
    "#np.save('X_test_count', X_test_count)#Saving results\n",
    "\n",
    "\n",
    "#N-gram\n",
    "ngram_vect = CountVectorizer(ngram_range=(1,3)) #Running N-Gram on full dataset (Max = 3)\n",
    "X_train = X_train.apply(lambda x: ' '.join(map(str,x)))\n",
    "ngram_vect_fit = ngram_vect.fit(X_train)\n",
    "\n",
    "ngram_train = ngram_vect_fit.transform(X_train)\n",
    "ngram_test = ngram_vect_fit.transform(X_test)\n",
    "\n",
    "X_train_ngram = pd.DataFrame(ngram_train.toarray())\n",
    "X_test_ngram = pd.DataFrame(ngram_test.toarray())\n",
    "\n",
    "np.save('X_train_ngram', X_train_count)#Saving results\n",
    "np.save('X_test_ngram', X_test_count)#Saving results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "#Saving results from previous step:\n",
    "np.save('X_train_tfidf', X_test_tfidf)\n",
    "np.save('X_test_tfidf', X_test_tfidf)\n",
    "\n",
    "np.save('X_train_count', X_train_count)\n",
    "np.save('X_test_count', X_test_count)\n",
    "\n",
    "np.save('X_train_ngram', X_train_ngram)\n",
    "np.save('X_test_ngram', X_test_ngram)\n",
    "\n",
    "np.save('Y_train', Y_train)\n",
    "np.save('Y_test', Y_test)"
   ]
  },
  {
   "source": [
    "## 5-1: Random Forest Model\n",
    "### 5-1-1: Explorering Random Forest with Holdout test set + grid-search\n",
    "#### 5-1-1-1 : Split data into training and test set\n",
    "#### 5-1-1-2 : Train vectorizes on training set and use that to transform test set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Being Introverts using TFIDF: Estimators: 10 / Max_Depth: 10 --> Precision: 0.768 / Recall: 1.0 / Accuracy: 0.768\n",
      "Being Introverts using TFIDF: Estimators: 10 / Max_Depth: 20 --> Precision: 0.768 / Recall: 1.0 / Accuracy: 0.768\n",
      "Being Introverts using TFIDF: Estimators: 10 / Max_Depth: 30 --> Precision: 0.769 / Recall: 0.996 / Accuracy: 0.768\n",
      "Being Introverts using TFIDF: Estimators: 10 / Max_Depth: None --> Precision: 0.782 / Recall: 0.953 / Accuracy: 0.76\n",
      "Being Introverts using TFIDF: Estimators: 50 / Max_Depth: 10 --> Precision: 0.768 / Recall: 1.0 / Accuracy: 0.768\n",
      "Being Introverts using TFIDF: Estimators: 50 / Max_Depth: 20 --> Precision: 0.768 / Recall: 1.0 / Accuracy: 0.768\n",
      "Being Introverts using TFIDF: Estimators: 50 / Max_Depth: 30 --> Precision: 0.768 / Recall: 1.0 / Accuracy: 0.768\n",
      "Being Introverts using TFIDF: Estimators: 50 / Max_Depth: None --> Precision: 0.769 / Recall: 1.0 / Accuracy: 0.769\n",
      "Being Introverts using TFIDF: Estimators: 100 / Max_Depth: 10 --> Precision: 0.768 / Recall: 1.0 / Accuracy: 0.768\n",
      "Being Introverts using TFIDF: Estimators: 100 / Max_Depth: 20 --> Precision: 0.768 / Recall: 1.0 / Accuracy: 0.768\n",
      "Being Introverts using TFIDF: Estimators: 100 / Max_Depth: 30 --> Precision: 0.768 / Recall: 1.0 / Accuracy: 0.768\n",
      "Being Introverts using TFIDF: Estimators: 100 / Max_Depth: None --> Precision: 0.768 / Recall: 1.0 / Accuracy: 0.768\n",
      "Being Intuitives using TFIDF: Estimators: 10 / Max_Depth: 10 --> Precision: 0.859 / Recall: 1.0 / Accuracy: 0.859\n",
      "Being Intuitives using TFIDF: Estimators: 10 / Max_Depth: 20 --> Precision: 0.859 / Recall: 1.0 / Accuracy: 0.859\n",
      "Being Intuitives using TFIDF: Estimators: 10 / Max_Depth: 30 --> Precision: 0.86 / Recall: 0.999 / Accuracy: 0.859\n",
      "Being Intuitives using TFIDF: Estimators: 10 / Max_Depth: None --> Precision: 0.86 / Recall: 0.999 / Accuracy: 0.859\n",
      "Being Intuitives using TFIDF: Estimators: 50 / Max_Depth: 10 --> Precision: 0.859 / Recall: 1.0 / Accuracy: 0.859\n",
      "Being Intuitives using TFIDF: Estimators: 50 / Max_Depth: 20 --> Precision: 0.859 / Recall: 1.0 / Accuracy: 0.859\n",
      "Being Intuitives using TFIDF: Estimators: 50 / Max_Depth: 30 --> Precision: 0.859 / Recall: 1.0 / Accuracy: 0.859\n",
      "Being Intuitives using TFIDF: Estimators: 50 / Max_Depth: None --> Precision: 0.859 / Recall: 1.0 / Accuracy: 0.859\n",
      "Being Intuitives using TFIDF: Estimators: 100 / Max_Depth: 10 --> Precision: 0.859 / Recall: 1.0 / Accuracy: 0.859\n",
      "Being Intuitives using TFIDF: Estimators: 100 / Max_Depth: 20 --> Precision: 0.859 / Recall: 1.0 / Accuracy: 0.859\n",
      "Being Intuitives using TFIDF: Estimators: 100 / Max_Depth: 30 --> Precision: 0.859 / Recall: 1.0 / Accuracy: 0.859\n",
      "Being Intuitives using TFIDF: Estimators: 100 / Max_Depth: None --> Precision: 0.859 / Recall: 1.0 / Accuracy: 0.859\n",
      "Being Feelers using TFIDF: Estimators: 10 / Max_Depth: 10 --> Precision: 0.617 / Recall: 0.839 / Accuracy: 0.631\n",
      "Being Feelers using TFIDF: Estimators: 10 / Max_Depth: 20 --> Precision: 0.647 / Recall: 0.758 / Accuracy: 0.645\n",
      "Being Feelers using TFIDF: Estimators: 10 / Max_Depth: 30 --> Precision: 0.646 / Recall: 0.717 / Accuracy: 0.634\n",
      "Being Feelers using TFIDF: Estimators: 10 / Max_Depth: None --> Precision: 0.632 / Recall: 0.805 / Accuracy: 0.641\n",
      "Being Feelers using TFIDF: Estimators: 50 / Max_Depth: 10 --> Precision: 0.638 / Recall: 0.94 / Accuracy: 0.679\n",
      "Being Feelers using TFIDF: Estimators: 50 / Max_Depth: 20 --> Precision: 0.678 / Recall: 0.889 / Accuracy: 0.712\n",
      "Being Feelers using TFIDF: Estimators: 50 / Max_Depth: 30 --> Precision: 0.699 / Recall: 0.85 / Accuracy: 0.721\n",
      "Being Feelers using TFIDF: Estimators: 50 / Max_Depth: None --> Precision: 0.695 / Recall: 0.87 / Accuracy: 0.723\n",
      "Being Feelers using TFIDF: Estimators: 100 / Max_Depth: 10 --> Precision: 0.631 / Recall: 0.953 / Accuracy: 0.673\n",
      "Being Feelers using TFIDF: Estimators: 100 / Max_Depth: 20 --> Precision: 0.675 / Recall: 0.884 / Accuracy: 0.707\n",
      "Being Feelers using TFIDF: Estimators: 100 / Max_Depth: 30 --> Precision: 0.7 / Recall: 0.884 / Accuracy: 0.733\n",
      "Being Feelers using TFIDF: Estimators: 100 / Max_Depth: None --> Precision: 0.697 / Recall: 0.875 / Accuracy: 0.727\n",
      "Being Perceivers using TFIDF: Estimators: 10 / Max_Depth: 10 --> Precision: 0.618 / Recall: 0.97 / Accuracy: 0.612\n",
      "Being Perceivers using TFIDF: Estimators: 10 / Max_Depth: 20 --> Precision: 0.638 / Recall: 0.891 / Accuracy: 0.622\n",
      "Being Perceivers using TFIDF: Estimators: 10 / Max_Depth: 30 --> Precision: 0.631 / Recall: 0.833 / Accuracy: 0.597\n",
      "Being Perceivers using TFIDF: Estimators: 10 / Max_Depth: None --> Precision: 0.633 / Recall: 0.666 / Accuracy: 0.557\n",
      "Being Perceivers using TFIDF: Estimators: 50 / Max_Depth: 10 --> Precision: 0.617 / Recall: 1.0 / Accuracy: 0.617\n",
      "Being Perceivers using TFIDF: Estimators: 50 / Max_Depth: 20 --> Precision: 0.63 / Recall: 0.99 / Accuracy: 0.636\n",
      "Being Perceivers using TFIDF: Estimators: 50 / Max_Depth: 30 --> Precision: 0.635 / Recall: 0.967 / Accuracy: 0.637\n",
      "Being Perceivers using TFIDF: Estimators: 50 / Max_Depth: None --> Precision: 0.652 / Recall: 0.917 / Accuracy: 0.648\n",
      "Being Perceivers using TFIDF: Estimators: 100 / Max_Depth: 10 --> Precision: 0.616 / Recall: 1.0 / Accuracy: 0.616\n",
      "Being Perceivers using TFIDF: Estimators: 100 / Max_Depth: 20 --> Precision: 0.62 / Recall: 0.998 / Accuracy: 0.621\n",
      "Being Perceivers using TFIDF: Estimators: 100 / Max_Depth: 30 --> Precision: 0.622 / Recall: 0.993 / Accuracy: 0.624\n",
      "Being Perceivers using TFIDF: Estimators: 100 / Max_Depth: None --> Precision: 0.634 / Recall: 0.956 / Accuracy: 0.633\n",
      "Being Introverts using CountVectorizer: Estimators: 10 / Max_Depth: 10 --> Precision: 0.768 / Recall: 1.0 / Accuracy: 0.768\n",
      "Being Introverts using CountVectorizer: Estimators: 10 / Max_Depth: 20 --> Precision: 0.769 / Recall: 1.0 / Accuracy: 0.769\n",
      "Being Introverts using CountVectorizer: Estimators: 10 / Max_Depth: 30 --> Precision: 0.768 / Recall: 0.999 / Accuracy: 0.768\n",
      "Being Introverts using CountVectorizer: Estimators: 10 / Max_Depth: None --> Precision: 0.773 / Recall: 0.98 / Accuracy: 0.764\n",
      "Being Introverts using CountVectorizer: Estimators: 50 / Max_Depth: 10 --> Precision: 0.768 / Recall: 1.0 / Accuracy: 0.768\n",
      "Being Introverts using CountVectorizer: Estimators: 50 / Max_Depth: 20 --> Precision: 0.768 / Recall: 1.0 / Accuracy: 0.768\n",
      "Being Introverts using CountVectorizer: Estimators: 50 / Max_Depth: 30 --> Precision: 0.768 / Recall: 1.0 / Accuracy: 0.768\n",
      "Being Introverts using CountVectorizer: Estimators: 50 / Max_Depth: None --> Precision: 0.768 / Recall: 1.0 / Accuracy: 0.768\n",
      "Being Introverts using CountVectorizer: Estimators: 100 / Max_Depth: 10 --> Precision: 0.768 / Recall: 1.0 / Accuracy: 0.768\n",
      "Being Introverts using CountVectorizer: Estimators: 100 / Max_Depth: 20 --> Precision: 0.768 / Recall: 1.0 / Accuracy: 0.768\n",
      "Being Introverts using CountVectorizer: Estimators: 100 / Max_Depth: 30 --> Precision: 0.768 / Recall: 1.0 / Accuracy: 0.768\n",
      "Being Introverts using CountVectorizer: Estimators: 100 / Max_Depth: None --> Precision: 0.768 / Recall: 1.0 / Accuracy: 0.768\n",
      "Being Intuitives using CountVectorizer: Estimators: 10 / Max_Depth: 10 --> Precision: 0.859 / Recall: 1.0 / Accuracy: 0.859\n",
      "Being Intuitives using CountVectorizer: Estimators: 10 / Max_Depth: 20 --> Precision: 0.859 / Recall: 1.0 / Accuracy: 0.859\n",
      "Being Intuitives using CountVectorizer: Estimators: 10 / Max_Depth: 30 --> Precision: 0.859 / Recall: 1.0 / Accuracy: 0.859\n",
      "Being Intuitives using CountVectorizer: Estimators: 10 / Max_Depth: None --> Precision: 0.859 / Recall: 1.0 / Accuracy: 0.859\n",
      "Being Intuitives using CountVectorizer: Estimators: 50 / Max_Depth: 10 --> Precision: 0.859 / Recall: 1.0 / Accuracy: 0.859\n",
      "Being Intuitives using CountVectorizer: Estimators: 50 / Max_Depth: 20 --> Precision: 0.859 / Recall: 1.0 / Accuracy: 0.859\n",
      "Being Intuitives using CountVectorizer: Estimators: 50 / Max_Depth: 30 --> Precision: 0.859 / Recall: 1.0 / Accuracy: 0.859\n",
      "Being Intuitives using CountVectorizer: Estimators: 50 / Max_Depth: None --> Precision: 0.859 / Recall: 1.0 / Accuracy: 0.859\n",
      "Being Intuitives using CountVectorizer: Estimators: 100 / Max_Depth: 10 --> Precision: 0.859 / Recall: 1.0 / Accuracy: 0.859\n",
      "Being Intuitives using CountVectorizer: Estimators: 100 / Max_Depth: 20 --> Precision: 0.859 / Recall: 1.0 / Accuracy: 0.859\n",
      "Being Intuitives using CountVectorizer: Estimators: 100 / Max_Depth: 30 --> Precision: 0.859 / Recall: 1.0 / Accuracy: 0.859\n",
      "Being Intuitives using CountVectorizer: Estimators: 100 / Max_Depth: None --> Precision: 0.859 / Recall: 1.0 / Accuracy: 0.859\n",
      "Being Feelers using CountVectorizer: Estimators: 10 / Max_Depth: 10 --> Precision: 0.625 / Recall: 0.856 / Accuracy: 0.644\n",
      "Being Feelers using CountVectorizer: Estimators: 10 / Max_Depth: 20 --> Precision: 0.652 / Recall: 0.764 / Accuracy: 0.651\n",
      "Being Feelers using CountVectorizer: Estimators: 10 / Max_Depth: 30 --> Precision: 0.639 / Recall: 0.751 / Accuracy: 0.636\n",
      "Being Feelers using CountVectorizer: Estimators: 10 / Max_Depth: None --> Precision: 0.631 / Recall: 0.818 / Accuracy: 0.642\n",
      "Being Feelers using CountVectorizer: Estimators: 50 / Max_Depth: 10 --> Precision: 0.647 / Recall: 0.945 / Accuracy: 0.691\n",
      "Being Feelers using CountVectorizer: Estimators: 50 / Max_Depth: 20 --> Precision: 0.684 / Recall: 0.884 / Accuracy: 0.716\n",
      "Being Feelers using CountVectorizer: Estimators: 50 / Max_Depth: 30 --> Precision: 0.691 / Recall: 0.888 / Accuracy: 0.724\n",
      "Being Feelers using CountVectorizer: Estimators: 50 / Max_Depth: None --> Precision: 0.689 / Recall: 0.881 / Accuracy: 0.72\n",
      "Being Feelers using CountVectorizer: Estimators: 100 / Max_Depth: 10 --> Precision: 0.612 / Recall: 0.961 / Accuracy: 0.65\n",
      "Being Feelers using CountVectorizer: Estimators: 100 / Max_Depth: 20 --> Precision: 0.692 / Recall: 0.913 / Accuracy: 0.733\n",
      "Being Feelers using CountVectorizer: Estimators: 100 / Max_Depth: 30 --> Precision: 0.709 / Recall: 0.903 / Accuracy: 0.747\n",
      "Being Feelers using CountVectorizer: Estimators: 100 / Max_Depth: None --> Precision: 0.709 / Recall: 0.902 / Accuracy: 0.747\n",
      "Being Perceivers using CountVectorizer: Estimators: 10 / Max_Depth: 10 --> Precision: 0.622 / Recall: 0.984 / Accuracy: 0.622\n",
      "Being Perceivers using CountVectorizer: Estimators: 10 / Max_Depth: 20 --> Precision: 0.632 / Recall: 0.931 / Accuracy: 0.624\n",
      "Being Perceivers using CountVectorizer: Estimators: 10 / Max_Depth: 30 --> Precision: 0.627 / Recall: 0.875 / Accuracy: 0.602\n",
      "Being Perceivers using CountVectorizer: Estimators: 10 / Max_Depth: None --> Precision: 0.642 / Recall: 0.734 / Accuracy: 0.584\n",
      "Being Perceivers using CountVectorizer: Estimators: 50 / Max_Depth: 10 --> Precision: 0.616 / Recall: 1.0 / Accuracy: 0.617\n",
      "Being Perceivers using CountVectorizer: Estimators: 50 / Max_Depth: 20 --> Precision: 0.623 / Recall: 0.996 / Accuracy: 0.626\n",
      "Being Perceivers using CountVectorizer: Estimators: 50 / Max_Depth: 30 --> Precision: 0.631 / Recall: 0.986 / Accuracy: 0.636\n",
      "Being Perceivers using CountVectorizer: Estimators: 50 / Max_Depth: None --> Precision: 0.643 / Recall: 0.948 / Accuracy: 0.643\n",
      "Being Perceivers using CountVectorizer: Estimators: 100 / Max_Depth: 10 --> Precision: 0.616 / Recall: 1.0 / Accuracy: 0.616\n",
      "Being Perceivers using CountVectorizer: Estimators: 100 / Max_Depth: 20 --> Precision: 0.618 / Recall: 0.998 / Accuracy: 0.618\n",
      "Being Perceivers using CountVectorizer: Estimators: 100 / Max_Depth: 30 --> Precision: 0.624 / Recall: 0.995 / Accuracy: 0.628\n",
      "Being Perceivers using CountVectorizer: Estimators: 100 / Max_Depth: None --> Precision: 0.634 / Recall: 0.98 / Accuracy: 0.64\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          Type           Method  Estimators  Max_Depth  Precision    Recall  \\\n",
       "0   Introverts            TFIDF          10       10.0   0.767723  1.000000   \n",
       "1   Introverts            TFIDF          10       20.0   0.768166  1.000000   \n",
       "2   Introverts            TFIDF          10       30.0   0.769275  0.996246   \n",
       "3   Introverts            TFIDF          10        NaN   0.781538  0.953453   \n",
       "4   Introverts            TFIDF          50       10.0   0.767723  1.000000   \n",
       "5   Introverts            TFIDF          50       20.0   0.767723  1.000000   \n",
       "6   Introverts            TFIDF          50       30.0   0.767723  1.000000   \n",
       "7   Introverts            TFIDF          50        NaN   0.768609  1.000000   \n",
       "8   Introverts            TFIDF         100       10.0   0.767723  1.000000   \n",
       "9   Introverts            TFIDF         100       20.0   0.767723  1.000000   \n",
       "10  Introverts            TFIDF         100       30.0   0.767723  1.000000   \n",
       "11  Introverts            TFIDF         100        NaN   0.767723  1.000000   \n",
       "12  Intuitives            TFIDF          10       10.0   0.859366  1.000000   \n",
       "13  Intuitives            TFIDF          10       20.0   0.859366  1.000000   \n",
       "14  Intuitives            TFIDF          10       30.0   0.859781  0.999329   \n",
       "15  Intuitives            TFIDF          10        NaN   0.860196  0.998659   \n",
       "16  Intuitives            TFIDF          50       10.0   0.859366  1.000000   \n",
       "17  Intuitives            TFIDF          50       20.0   0.859366  1.000000   \n",
       "18  Intuitives            TFIDF          50       30.0   0.859366  1.000000   \n",
       "19  Intuitives            TFIDF          50        NaN   0.859366  1.000000   \n",
       "20  Intuitives            TFIDF         100       10.0   0.859366  1.000000   \n",
       "21  Intuitives            TFIDF         100       20.0   0.859366  1.000000   \n",
       "22  Intuitives            TFIDF         100       30.0   0.859366  1.000000   \n",
       "23  Intuitives            TFIDF         100        NaN   0.859366  1.000000   \n",
       "24     Feelers            TFIDF          10       10.0   0.617071  0.839191   \n",
       "25     Feelers            TFIDF          10       20.0   0.646685  0.758253   \n",
       "26     Feelers            TFIDF          10       30.0   0.645873  0.716720   \n",
       "27     Feelers            TFIDF          10        NaN   0.632107  0.805112   \n",
       "28     Feelers            TFIDF          50       10.0   0.638006  0.940362   \n",
       "29     Feelers            TFIDF          50       20.0   0.678310  0.889244   \n",
       "30     Feelers            TFIDF          50       30.0   0.699387  0.849840   \n",
       "31     Feelers            TFIDF          50        NaN   0.694728  0.870075   \n",
       "32     Feelers            TFIDF         100       10.0   0.630726  0.953142   \n",
       "33     Feelers            TFIDF         100       20.0   0.674797  0.883919   \n",
       "34     Feelers            TFIDF         100       30.0   0.700422  0.883919   \n",
       "35     Feelers            TFIDF         100        NaN   0.697201  0.875399   \n",
       "36  Perceivers            TFIDF          10       10.0   0.617998  0.970065   \n",
       "37  Perceivers            TFIDF          10       20.0   0.638498  0.890552   \n",
       "38  Perceivers            TFIDF          10       30.0   0.631020  0.833489   \n",
       "39  Perceivers            TFIDF          10        NaN   0.633452  0.666043   \n",
       "40  Perceivers            TFIDF          50       10.0   0.616849  1.000000   \n",
       "41  Perceivers            TFIDF          50       20.0   0.630137  0.989710   \n",
       "42  Perceivers            TFIDF          50       30.0   0.634745  0.967259   \n",
       "43  Perceivers            TFIDF          50        NaN   0.652463  0.916745   \n",
       "44  Perceivers            TFIDF         100       10.0   0.616138  1.000000   \n",
       "45  Perceivers            TFIDF         100       20.0   0.619628  0.998129   \n",
       "46  Perceivers            TFIDF         100       30.0   0.622287  0.992516   \n",
       "47  Perceivers            TFIDF         100        NaN   0.633995  0.956034   \n",
       "48  Introverts  CountVectorizer          10       10.0   0.767723  1.000000   \n",
       "49  Introverts  CountVectorizer          10       20.0   0.768609  1.000000   \n",
       "50  Introverts  CountVectorizer          10       30.0   0.768476  0.999249   \n",
       "51  Introverts  CountVectorizer          10        NaN   0.772781  0.980480   \n",
       "52  Introverts  CountVectorizer          50       10.0   0.767723  1.000000   \n",
       "53  Introverts  CountVectorizer          50       20.0   0.767723  1.000000   \n",
       "54  Introverts  CountVectorizer          50       30.0   0.767723  1.000000   \n",
       "55  Introverts  CountVectorizer          50        NaN   0.768166  1.000000   \n",
       "56  Introverts  CountVectorizer         100       10.0   0.767723  1.000000   \n",
       "57  Introverts  CountVectorizer         100       20.0   0.767723  1.000000   \n",
       "58  Introverts  CountVectorizer         100       30.0   0.767723  1.000000   \n",
       "59  Introverts  CountVectorizer         100        NaN   0.767723  1.000000   \n",
       "60  Intuitives  CountVectorizer          10       10.0   0.859366  1.000000   \n",
       "61  Intuitives  CountVectorizer          10       20.0   0.859366  1.000000   \n",
       "62  Intuitives  CountVectorizer          10       30.0   0.859366  1.000000   \n",
       "63  Intuitives  CountVectorizer          10        NaN   0.859366  1.000000   \n",
       "64  Intuitives  CountVectorizer          50       10.0   0.859366  1.000000   \n",
       "65  Intuitives  CountVectorizer          50       20.0   0.859366  1.000000   \n",
       "66  Intuitives  CountVectorizer          50       30.0   0.859366  1.000000   \n",
       "67  Intuitives  CountVectorizer          50        NaN   0.859366  1.000000   \n",
       "68  Intuitives  CountVectorizer         100       10.0   0.859366  1.000000   \n",
       "69  Intuitives  CountVectorizer         100       20.0   0.859366  1.000000   \n",
       "70  Intuitives  CountVectorizer         100       30.0   0.859366  1.000000   \n",
       "71  Intuitives  CountVectorizer         100        NaN   0.859366  1.000000   \n",
       "72     Feelers  CountVectorizer          10       10.0   0.624709  0.856230   \n",
       "73     Feelers  CountVectorizer          10       20.0   0.651818  0.763578   \n",
       "74     Feelers  CountVectorizer          10       30.0   0.639166  0.750799   \n",
       "75     Feelers  CountVectorizer          10        NaN   0.630542  0.817891   \n",
       "76     Feelers  CountVectorizer          50       10.0   0.646973  0.944622   \n",
       "77     Feelers  CountVectorizer          50       20.0   0.683690  0.883919   \n",
       "78     Feelers  CountVectorizer          50       30.0   0.690969  0.888179   \n",
       "79     Feelers  CountVectorizer          50        NaN   0.688593  0.880724   \n",
       "80     Feelers  CountVectorizer         100       10.0   0.612356  0.960596   \n",
       "81     Feelers  CountVectorizer         100       20.0   0.692246  0.912673   \n",
       "82     Feelers  CountVectorizer         100       30.0   0.709030  0.903088   \n",
       "83     Feelers  CountVectorizer         100        NaN   0.709380  0.902023   \n",
       "84  Perceivers  CountVectorizer          10       10.0   0.622117  0.984097   \n",
       "85  Perceivers  CountVectorizer          10       20.0   0.632147  0.930776   \n",
       "86  Perceivers  CountVectorizer          10       30.0   0.627096  0.874649   \n",
       "87  Perceivers  CountVectorizer          10        NaN   0.641864  0.734331   \n",
       "88  Perceivers  CountVectorizer          50       10.0   0.616494  1.000000   \n",
       "89  Perceivers  CountVectorizer          50       20.0   0.622807  0.996258   \n",
       "90  Perceivers  CountVectorizer          50       30.0   0.631138  0.985968   \n",
       "91  Perceivers  CountVectorizer          50        NaN   0.642766  0.947615   \n",
       "92  Perceivers  CountVectorizer         100       10.0   0.616138  1.000000   \n",
       "93  Perceivers  CountVectorizer         100       20.0   0.617834  0.998129   \n",
       "94  Perceivers  CountVectorizer         100       30.0   0.624413  0.995323   \n",
       "95  Perceivers  CountVectorizer         100        NaN   0.634383  0.980355   \n",
       "\n",
       "    Accuracy  \n",
       "0      0.768  \n",
       "1      0.768  \n",
       "2      0.768  \n",
       "3      0.760  \n",
       "4      0.768  \n",
       "5      0.768  \n",
       "6      0.768  \n",
       "7      0.769  \n",
       "8      0.768  \n",
       "9      0.768  \n",
       "10     0.768  \n",
       "11     0.768  \n",
       "12     0.859  \n",
       "13     0.859  \n",
       "14     0.859  \n",
       "15     0.859  \n",
       "16     0.859  \n",
       "17     0.859  \n",
       "18     0.859  \n",
       "19     0.859  \n",
       "20     0.859  \n",
       "21     0.859  \n",
       "22     0.859  \n",
       "23     0.859  \n",
       "24     0.631  \n",
       "25     0.645  \n",
       "26     0.634  \n",
       "27     0.641  \n",
       "28     0.679  \n",
       "29     0.712  \n",
       "30     0.721  \n",
       "31     0.723  \n",
       "32     0.673  \n",
       "33     0.707  \n",
       "34     0.733  \n",
       "35     0.727  \n",
       "36     0.612  \n",
       "37     0.622  \n",
       "38     0.597  \n",
       "39     0.557  \n",
       "40     0.617  \n",
       "41     0.636  \n",
       "42     0.637  \n",
       "43     0.648  \n",
       "44     0.616  \n",
       "45     0.621  \n",
       "46     0.624  \n",
       "47     0.633  \n",
       "48     0.768  \n",
       "49     0.769  \n",
       "50     0.768  \n",
       "51     0.764  \n",
       "52     0.768  \n",
       "53     0.768  \n",
       "54     0.768  \n",
       "55     0.768  \n",
       "56     0.768  \n",
       "57     0.768  \n",
       "58     0.768  \n",
       "59     0.768  \n",
       "60     0.859  \n",
       "61     0.859  \n",
       "62     0.859  \n",
       "63     0.859  \n",
       "64     0.859  \n",
       "65     0.859  \n",
       "66     0.859  \n",
       "67     0.859  \n",
       "68     0.859  \n",
       "69     0.859  \n",
       "70     0.859  \n",
       "71     0.859  \n",
       "72     0.644  \n",
       "73     0.651  \n",
       "74     0.636  \n",
       "75     0.642  \n",
       "76     0.691  \n",
       "77     0.716  \n",
       "78     0.724  \n",
       "79     0.720  \n",
       "80     0.650  \n",
       "81     0.733  \n",
       "82     0.747  \n",
       "83     0.747  \n",
       "84     0.622  \n",
       "85     0.624  \n",
       "86     0.602  \n",
       "87     0.584  \n",
       "88     0.617  \n",
       "89     0.626  \n",
       "90     0.636  \n",
       "91     0.643  \n",
       "92     0.616  \n",
       "93     0.618  \n",
       "94     0.628  \n",
       "95     0.640  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Type</th>\n      <th>Method</th>\n      <th>Estimators</th>\n      <th>Max_Depth</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Introverts</td>\n      <td>TFIDF</td>\n      <td>10</td>\n      <td>10.0</td>\n      <td>0.767723</td>\n      <td>1.000000</td>\n      <td>0.768</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Introverts</td>\n      <td>TFIDF</td>\n      <td>10</td>\n      <td>20.0</td>\n      <td>0.768166</td>\n      <td>1.000000</td>\n      <td>0.768</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Introverts</td>\n      <td>TFIDF</td>\n      <td>10</td>\n      <td>30.0</td>\n      <td>0.769275</td>\n      <td>0.996246</td>\n      <td>0.768</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Introverts</td>\n      <td>TFIDF</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>0.781538</td>\n      <td>0.953453</td>\n      <td>0.760</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Introverts</td>\n      <td>TFIDF</td>\n      <td>50</td>\n      <td>10.0</td>\n      <td>0.767723</td>\n      <td>1.000000</td>\n      <td>0.768</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Introverts</td>\n      <td>TFIDF</td>\n      <td>50</td>\n      <td>20.0</td>\n      <td>0.767723</td>\n      <td>1.000000</td>\n      <td>0.768</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Introverts</td>\n      <td>TFIDF</td>\n      <td>50</td>\n      <td>30.0</td>\n      <td>0.767723</td>\n      <td>1.000000</td>\n      <td>0.768</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Introverts</td>\n      <td>TFIDF</td>\n      <td>50</td>\n      <td>NaN</td>\n      <td>0.768609</td>\n      <td>1.000000</td>\n      <td>0.769</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Introverts</td>\n      <td>TFIDF</td>\n      <td>100</td>\n      <td>10.0</td>\n      <td>0.767723</td>\n      <td>1.000000</td>\n      <td>0.768</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Introverts</td>\n      <td>TFIDF</td>\n      <td>100</td>\n      <td>20.0</td>\n      <td>0.767723</td>\n      <td>1.000000</td>\n      <td>0.768</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Introverts</td>\n      <td>TFIDF</td>\n      <td>100</td>\n      <td>30.0</td>\n      <td>0.767723</td>\n      <td>1.000000</td>\n      <td>0.768</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Introverts</td>\n      <td>TFIDF</td>\n      <td>100</td>\n      <td>NaN</td>\n      <td>0.767723</td>\n      <td>1.000000</td>\n      <td>0.768</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Intuitives</td>\n      <td>TFIDF</td>\n      <td>10</td>\n      <td>10.0</td>\n      <td>0.859366</td>\n      <td>1.000000</td>\n      <td>0.859</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Intuitives</td>\n      <td>TFIDF</td>\n      <td>10</td>\n      <td>20.0</td>\n      <td>0.859366</td>\n      <td>1.000000</td>\n      <td>0.859</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Intuitives</td>\n      <td>TFIDF</td>\n      <td>10</td>\n      <td>30.0</td>\n      <td>0.859781</td>\n      <td>0.999329</td>\n      <td>0.859</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Intuitives</td>\n      <td>TFIDF</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>0.860196</td>\n      <td>0.998659</td>\n      <td>0.859</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>Intuitives</td>\n      <td>TFIDF</td>\n      <td>50</td>\n      <td>10.0</td>\n      <td>0.859366</td>\n      <td>1.000000</td>\n      <td>0.859</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Intuitives</td>\n      <td>TFIDF</td>\n      <td>50</td>\n      <td>20.0</td>\n      <td>0.859366</td>\n      <td>1.000000</td>\n      <td>0.859</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>Intuitives</td>\n      <td>TFIDF</td>\n      <td>50</td>\n      <td>30.0</td>\n      <td>0.859366</td>\n      <td>1.000000</td>\n      <td>0.859</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Intuitives</td>\n      <td>TFIDF</td>\n      <td>50</td>\n      <td>NaN</td>\n      <td>0.859366</td>\n      <td>1.000000</td>\n      <td>0.859</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Intuitives</td>\n      <td>TFIDF</td>\n      <td>100</td>\n      <td>10.0</td>\n      <td>0.859366</td>\n      <td>1.000000</td>\n      <td>0.859</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>Intuitives</td>\n      <td>TFIDF</td>\n      <td>100</td>\n      <td>20.0</td>\n      <td>0.859366</td>\n      <td>1.000000</td>\n      <td>0.859</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>Intuitives</td>\n      <td>TFIDF</td>\n      <td>100</td>\n      <td>30.0</td>\n      <td>0.859366</td>\n      <td>1.000000</td>\n      <td>0.859</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>Intuitives</td>\n      <td>TFIDF</td>\n      <td>100</td>\n      <td>NaN</td>\n      <td>0.859366</td>\n      <td>1.000000</td>\n      <td>0.859</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>Feelers</td>\n      <td>TFIDF</td>\n      <td>10</td>\n      <td>10.0</td>\n      <td>0.617071</td>\n      <td>0.839191</td>\n      <td>0.631</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>Feelers</td>\n      <td>TFIDF</td>\n      <td>10</td>\n      <td>20.0</td>\n      <td>0.646685</td>\n      <td>0.758253</td>\n      <td>0.645</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>Feelers</td>\n      <td>TFIDF</td>\n      <td>10</td>\n      <td>30.0</td>\n      <td>0.645873</td>\n      <td>0.716720</td>\n      <td>0.634</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>Feelers</td>\n      <td>TFIDF</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>0.632107</td>\n      <td>0.805112</td>\n      <td>0.641</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>Feelers</td>\n      <td>TFIDF</td>\n      <td>50</td>\n      <td>10.0</td>\n      <td>0.638006</td>\n      <td>0.940362</td>\n      <td>0.679</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>Feelers</td>\n      <td>TFIDF</td>\n      <td>50</td>\n      <td>20.0</td>\n      <td>0.678310</td>\n      <td>0.889244</td>\n      <td>0.712</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>Feelers</td>\n      <td>TFIDF</td>\n      <td>50</td>\n      <td>30.0</td>\n      <td>0.699387</td>\n      <td>0.849840</td>\n      <td>0.721</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>Feelers</td>\n      <td>TFIDF</td>\n      <td>50</td>\n      <td>NaN</td>\n      <td>0.694728</td>\n      <td>0.870075</td>\n      <td>0.723</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>Feelers</td>\n      <td>TFIDF</td>\n      <td>100</td>\n      <td>10.0</td>\n      <td>0.630726</td>\n      <td>0.953142</td>\n      <td>0.673</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>Feelers</td>\n      <td>TFIDF</td>\n      <td>100</td>\n      <td>20.0</td>\n      <td>0.674797</td>\n      <td>0.883919</td>\n      <td>0.707</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>Feelers</td>\n      <td>TFIDF</td>\n      <td>100</td>\n      <td>30.0</td>\n      <td>0.700422</td>\n      <td>0.883919</td>\n      <td>0.733</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>Feelers</td>\n      <td>TFIDF</td>\n      <td>100</td>\n      <td>NaN</td>\n      <td>0.697201</td>\n      <td>0.875399</td>\n      <td>0.727</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>Perceivers</td>\n      <td>TFIDF</td>\n      <td>10</td>\n      <td>10.0</td>\n      <td>0.617998</td>\n      <td>0.970065</td>\n      <td>0.612</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>Perceivers</td>\n      <td>TFIDF</td>\n      <td>10</td>\n      <td>20.0</td>\n      <td>0.638498</td>\n      <td>0.890552</td>\n      <td>0.622</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>Perceivers</td>\n      <td>TFIDF</td>\n      <td>10</td>\n      <td>30.0</td>\n      <td>0.631020</td>\n      <td>0.833489</td>\n      <td>0.597</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>Perceivers</td>\n      <td>TFIDF</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>0.633452</td>\n      <td>0.666043</td>\n      <td>0.557</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>Perceivers</td>\n      <td>TFIDF</td>\n      <td>50</td>\n      <td>10.0</td>\n      <td>0.616849</td>\n      <td>1.000000</td>\n      <td>0.617</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>Perceivers</td>\n      <td>TFIDF</td>\n      <td>50</td>\n      <td>20.0</td>\n      <td>0.630137</td>\n      <td>0.989710</td>\n      <td>0.636</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>Perceivers</td>\n      <td>TFIDF</td>\n      <td>50</td>\n      <td>30.0</td>\n      <td>0.634745</td>\n      <td>0.967259</td>\n      <td>0.637</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>Perceivers</td>\n      <td>TFIDF</td>\n      <td>50</td>\n      <td>NaN</td>\n      <td>0.652463</td>\n      <td>0.916745</td>\n      <td>0.648</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>Perceivers</td>\n      <td>TFIDF</td>\n      <td>100</td>\n      <td>10.0</td>\n      <td>0.616138</td>\n      <td>1.000000</td>\n      <td>0.616</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>Perceivers</td>\n      <td>TFIDF</td>\n      <td>100</td>\n      <td>20.0</td>\n      <td>0.619628</td>\n      <td>0.998129</td>\n      <td>0.621</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>Perceivers</td>\n      <td>TFIDF</td>\n      <td>100</td>\n      <td>30.0</td>\n      <td>0.622287</td>\n      <td>0.992516</td>\n      <td>0.624</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>Perceivers</td>\n      <td>TFIDF</td>\n      <td>100</td>\n      <td>NaN</td>\n      <td>0.633995</td>\n      <td>0.956034</td>\n      <td>0.633</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>Introverts</td>\n      <td>CountVectorizer</td>\n      <td>10</td>\n      <td>10.0</td>\n      <td>0.767723</td>\n      <td>1.000000</td>\n      <td>0.768</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>Introverts</td>\n      <td>CountVectorizer</td>\n      <td>10</td>\n      <td>20.0</td>\n      <td>0.768609</td>\n      <td>1.000000</td>\n      <td>0.769</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>Introverts</td>\n      <td>CountVectorizer</td>\n      <td>10</td>\n      <td>30.0</td>\n      <td>0.768476</td>\n      <td>0.999249</td>\n      <td>0.768</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>Introverts</td>\n      <td>CountVectorizer</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>0.772781</td>\n      <td>0.980480</td>\n      <td>0.764</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>Introverts</td>\n      <td>CountVectorizer</td>\n      <td>50</td>\n      <td>10.0</td>\n      <td>0.767723</td>\n      <td>1.000000</td>\n      <td>0.768</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>Introverts</td>\n      <td>CountVectorizer</td>\n      <td>50</td>\n      <td>20.0</td>\n      <td>0.767723</td>\n      <td>1.000000</td>\n      <td>0.768</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>Introverts</td>\n      <td>CountVectorizer</td>\n      <td>50</td>\n      <td>30.0</td>\n      <td>0.767723</td>\n      <td>1.000000</td>\n      <td>0.768</td>\n    </tr>\n    <tr>\n      <th>55</th>\n      <td>Introverts</td>\n      <td>CountVectorizer</td>\n      <td>50</td>\n      <td>NaN</td>\n      <td>0.768166</td>\n      <td>1.000000</td>\n      <td>0.768</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>Introverts</td>\n      <td>CountVectorizer</td>\n      <td>100</td>\n      <td>10.0</td>\n      <td>0.767723</td>\n      <td>1.000000</td>\n      <td>0.768</td>\n    </tr>\n    <tr>\n      <th>57</th>\n      <td>Introverts</td>\n      <td>CountVectorizer</td>\n      <td>100</td>\n      <td>20.0</td>\n      <td>0.767723</td>\n      <td>1.000000</td>\n      <td>0.768</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>Introverts</td>\n      <td>CountVectorizer</td>\n      <td>100</td>\n      <td>30.0</td>\n      <td>0.767723</td>\n      <td>1.000000</td>\n      <td>0.768</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>Introverts</td>\n      <td>CountVectorizer</td>\n      <td>100</td>\n      <td>NaN</td>\n      <td>0.767723</td>\n      <td>1.000000</td>\n      <td>0.768</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>Intuitives</td>\n      <td>CountVectorizer</td>\n      <td>10</td>\n      <td>10.0</td>\n      <td>0.859366</td>\n      <td>1.000000</td>\n      <td>0.859</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>Intuitives</td>\n      <td>CountVectorizer</td>\n      <td>10</td>\n      <td>20.0</td>\n      <td>0.859366</td>\n      <td>1.000000</td>\n      <td>0.859</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>Intuitives</td>\n      <td>CountVectorizer</td>\n      <td>10</td>\n      <td>30.0</td>\n      <td>0.859366</td>\n      <td>1.000000</td>\n      <td>0.859</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>Intuitives</td>\n      <td>CountVectorizer</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>0.859366</td>\n      <td>1.000000</td>\n      <td>0.859</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>Intuitives</td>\n      <td>CountVectorizer</td>\n      <td>50</td>\n      <td>10.0</td>\n      <td>0.859366</td>\n      <td>1.000000</td>\n      <td>0.859</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>Intuitives</td>\n      <td>CountVectorizer</td>\n      <td>50</td>\n      <td>20.0</td>\n      <td>0.859366</td>\n      <td>1.000000</td>\n      <td>0.859</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>Intuitives</td>\n      <td>CountVectorizer</td>\n      <td>50</td>\n      <td>30.0</td>\n      <td>0.859366</td>\n      <td>1.000000</td>\n      <td>0.859</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>Intuitives</td>\n      <td>CountVectorizer</td>\n      <td>50</td>\n      <td>NaN</td>\n      <td>0.859366</td>\n      <td>1.000000</td>\n      <td>0.859</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>Intuitives</td>\n      <td>CountVectorizer</td>\n      <td>100</td>\n      <td>10.0</td>\n      <td>0.859366</td>\n      <td>1.000000</td>\n      <td>0.859</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>Intuitives</td>\n      <td>CountVectorizer</td>\n      <td>100</td>\n      <td>20.0</td>\n      <td>0.859366</td>\n      <td>1.000000</td>\n      <td>0.859</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>Intuitives</td>\n      <td>CountVectorizer</td>\n      <td>100</td>\n      <td>30.0</td>\n      <td>0.859366</td>\n      <td>1.000000</td>\n      <td>0.859</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>Intuitives</td>\n      <td>CountVectorizer</td>\n      <td>100</td>\n      <td>NaN</td>\n      <td>0.859366</td>\n      <td>1.000000</td>\n      <td>0.859</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>Feelers</td>\n      <td>CountVectorizer</td>\n      <td>10</td>\n      <td>10.0</td>\n      <td>0.624709</td>\n      <td>0.856230</td>\n      <td>0.644</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>Feelers</td>\n      <td>CountVectorizer</td>\n      <td>10</td>\n      <td>20.0</td>\n      <td>0.651818</td>\n      <td>0.763578</td>\n      <td>0.651</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>Feelers</td>\n      <td>CountVectorizer</td>\n      <td>10</td>\n      <td>30.0</td>\n      <td>0.639166</td>\n      <td>0.750799</td>\n      <td>0.636</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>Feelers</td>\n      <td>CountVectorizer</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>0.630542</td>\n      <td>0.817891</td>\n      <td>0.642</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>Feelers</td>\n      <td>CountVectorizer</td>\n      <td>50</td>\n      <td>10.0</td>\n      <td>0.646973</td>\n      <td>0.944622</td>\n      <td>0.691</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>Feelers</td>\n      <td>CountVectorizer</td>\n      <td>50</td>\n      <td>20.0</td>\n      <td>0.683690</td>\n      <td>0.883919</td>\n      <td>0.716</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>Feelers</td>\n      <td>CountVectorizer</td>\n      <td>50</td>\n      <td>30.0</td>\n      <td>0.690969</td>\n      <td>0.888179</td>\n      <td>0.724</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>Feelers</td>\n      <td>CountVectorizer</td>\n      <td>50</td>\n      <td>NaN</td>\n      <td>0.688593</td>\n      <td>0.880724</td>\n      <td>0.720</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>Feelers</td>\n      <td>CountVectorizer</td>\n      <td>100</td>\n      <td>10.0</td>\n      <td>0.612356</td>\n      <td>0.960596</td>\n      <td>0.650</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>Feelers</td>\n      <td>CountVectorizer</td>\n      <td>100</td>\n      <td>20.0</td>\n      <td>0.692246</td>\n      <td>0.912673</td>\n      <td>0.733</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>Feelers</td>\n      <td>CountVectorizer</td>\n      <td>100</td>\n      <td>30.0</td>\n      <td>0.709030</td>\n      <td>0.903088</td>\n      <td>0.747</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>Feelers</td>\n      <td>CountVectorizer</td>\n      <td>100</td>\n      <td>NaN</td>\n      <td>0.709380</td>\n      <td>0.902023</td>\n      <td>0.747</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>Perceivers</td>\n      <td>CountVectorizer</td>\n      <td>10</td>\n      <td>10.0</td>\n      <td>0.622117</td>\n      <td>0.984097</td>\n      <td>0.622</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>Perceivers</td>\n      <td>CountVectorizer</td>\n      <td>10</td>\n      <td>20.0</td>\n      <td>0.632147</td>\n      <td>0.930776</td>\n      <td>0.624</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>Perceivers</td>\n      <td>CountVectorizer</td>\n      <td>10</td>\n      <td>30.0</td>\n      <td>0.627096</td>\n      <td>0.874649</td>\n      <td>0.602</td>\n    </tr>\n    <tr>\n      <th>87</th>\n      <td>Perceivers</td>\n      <td>CountVectorizer</td>\n      <td>10</td>\n      <td>NaN</td>\n      <td>0.641864</td>\n      <td>0.734331</td>\n      <td>0.584</td>\n    </tr>\n    <tr>\n      <th>88</th>\n      <td>Perceivers</td>\n      <td>CountVectorizer</td>\n      <td>50</td>\n      <td>10.0</td>\n      <td>0.616494</td>\n      <td>1.000000</td>\n      <td>0.617</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>Perceivers</td>\n      <td>CountVectorizer</td>\n      <td>50</td>\n      <td>20.0</td>\n      <td>0.622807</td>\n      <td>0.996258</td>\n      <td>0.626</td>\n    </tr>\n    <tr>\n      <th>90</th>\n      <td>Perceivers</td>\n      <td>CountVectorizer</td>\n      <td>50</td>\n      <td>30.0</td>\n      <td>0.631138</td>\n      <td>0.985968</td>\n      <td>0.636</td>\n    </tr>\n    <tr>\n      <th>91</th>\n      <td>Perceivers</td>\n      <td>CountVectorizer</td>\n      <td>50</td>\n      <td>NaN</td>\n      <td>0.642766</td>\n      <td>0.947615</td>\n      <td>0.643</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>Perceivers</td>\n      <td>CountVectorizer</td>\n      <td>100</td>\n      <td>10.0</td>\n      <td>0.616138</td>\n      <td>1.000000</td>\n      <td>0.616</td>\n    </tr>\n    <tr>\n      <th>93</th>\n      <td>Perceivers</td>\n      <td>CountVectorizer</td>\n      <td>100</td>\n      <td>20.0</td>\n      <td>0.617834</td>\n      <td>0.998129</td>\n      <td>0.618</td>\n    </tr>\n    <tr>\n      <th>94</th>\n      <td>Perceivers</td>\n      <td>CountVectorizer</td>\n      <td>100</td>\n      <td>30.0</td>\n      <td>0.624413</td>\n      <td>0.995323</td>\n      <td>0.628</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>Perceivers</td>\n      <td>CountVectorizer</td>\n      <td>100</td>\n      <td>NaN</td>\n      <td>0.634383</td>\n      <td>0.980355</td>\n      <td>0.640</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "pd.set_option('display.max_rows', None, 'display.max_colwidth', 500)\n",
    "\n",
    "\n",
    "def rfClassifier_GridSearch(X_test_set, X_train_set, Lable, n_est, depth, feature_type, is_print = True):\n",
    "    if Lable == 'IE':\n",
    "        s_Lable = 'I'\n",
    "        predict_Lable = 'Introverts'\n",
    "    elif Lable == 'NS':\n",
    "        s_Lable = 'N'\n",
    "        predict_Lable = 'Intuitives'\n",
    "    elif Lable == 'FT':\n",
    "        s_Lable = 'F'\n",
    "        predict_Lable = 'Feelers'\n",
    "    elif Lable == 'PJ':\n",
    "        s_Lable = 'P'\n",
    "        predict_Lable = 'Perceivers'\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_depth=depth ,n_jobs=-1)#Max depth of tree is 20\n",
    "    rf_model = rf.fit(X_train_set, Y_train[Lable])\n",
    "    Y_pred = rf_model.predict(X_test_set)\n",
    "\n",
    "    precision, recall, fscore, support = score(Y_test[Lable], Y_pred, pos_label=s_Lable, average='binary')\n",
    "    if is_print:\n",
    "        print('Being ' + predict_Lable + ' using ' + feature_type + ':'  ,\n",
    "        'Estimators: {} / Max_Depth: {} --> Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "                                                        n_est,\n",
    "                                                        depth,\n",
    "                                                        round(precision, 3),\n",
    "                                                        round(recall, 3), \n",
    "                                                        round((Y_pred==Y_test[Lable]).sum() / len(Y_pred),3)))\n",
    "    return([predict_Lable, feature_type, n_est, depth, precision, recall, round((Y_pred==Y_test[Lable]).sum() / len(Y_pred),3)])\n",
    "\n",
    "\n",
    "classes = ['IE' , 'NS', 'FT', 'PJ']\n",
    "rf_result = []\n",
    "Y_test = sparse.load_npz('Y_test.npz')\n",
    "Y_train = sparse.load_npz('Y_train.npz')\n",
    "\n",
    "for feature_type in ['TFIDF', 'CountVectorizer', 'N-gram']:\n",
    "    if feature_type == 'TFIDF':\n",
    "        X_test_set = sparse.load_npz('X_test_tfidf.npz')\n",
    "        X_train_set = sparse.load_npz('X_train_tfidf.npz')\n",
    "    elif feature_type == 'CountVectorizer':\n",
    "        X_test_set = sparse.load_npz('X_test_count.npz')\n",
    "        X_train_set = sparse.load_npz('X_train_count.npz')\n",
    "    elif feature_type == 'N-gram':\n",
    "        X_test_set = sparse.load_npz('X_test_ngram.npz')\n",
    "        X_train_set = sparse.load_npz('X_train_ngram.npz')\n",
    "    for item in classes:\n",
    "        for n_est in [10, 50, 100]:\n",
    "            for depth in [10, 20, 30, None]:\n",
    "                rf_result.append(rfClassifier_GridSearch(X_test_set = X_test_set, X_train_set = X_train_set, Lable = item, n_est=n_est, depth=depth, feature_type=feature_type,is_print = True))\n",
    "\n",
    "\n",
    "pd.DataFrame(rf_result, columns= ['Type', 'Method', 'Estimators', 'Max_Depth', 'Precision', 'Recall', 'Accuracy']).to_csv('RF_Holdout_Result.csv')\n",
    "pd.DataFrame(rf_result, columns= ['Type', 'Method', 'Estimators', 'Max_Depth', 'Precision', 'Recall', 'Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   param_max_depth param_n_estimators  std_test_score  mean_test_score  \\\n",
      "6               90                 10        0.004380         0.771873   \n",
      "0               30                 10        0.001094         0.770605   \n",
      "10            None                150        0.000565         0.770490   \n",
      "11            None                300        0.000461         0.770375   \n",
      "7               90                150        0.000461         0.770259   \n",
      "\n",
      "    rank_test_score  \n",
      "6                 1  \n",
      "0                 2  \n",
      "10                3  \n",
      "11                4  \n",
      "7                 5  \n",
      "Count Vectorizer\n",
      "   param_max_depth param_n_estimators  std_test_score  mean_test_score  \\\n",
      "10            None                150        0.000936         0.770259   \n",
      "8               90                300        0.000431         0.770144   \n",
      "11            None                300        0.000672         0.770144   \n",
      "4               60                150        0.000365         0.770029   \n",
      "5               60                300        0.000365         0.770029   \n",
      "\n",
      "    rank_test_score  \n",
      "10                1  \n",
      "8                 2  \n",
      "11                2  \n",
      "4                 4  \n",
      "5                 4  \n",
      "TFIDF Vectorizer\n",
      "  param_max_depth param_n_estimators  std_test_score  mean_test_score  \\\n",
      "6              90                 10        0.001314         0.862824   \n",
      "3              60                 10        0.000365         0.862248   \n",
      "0              30                 10        0.000282         0.862017   \n",
      "1              30                150        0.000282         0.862017   \n",
      "2              30                300        0.000282         0.862017   \n",
      "\n",
      "   rank_test_score  \n",
      "6                1  \n",
      "3                2  \n",
      "0                3  \n",
      "1                3  \n",
      "2                3  \n",
      "Count Vectorizer\n",
      "  param_max_depth param_n_estimators  std_test_score  mean_test_score  \\\n",
      "3              60                 10        0.000863         0.862478   \n",
      "9            None                 10        0.000565         0.862363   \n",
      "1              30                150        0.000282         0.862017   \n",
      "2              30                300        0.000282         0.862017   \n",
      "4              60                150        0.000282         0.862017   \n",
      "\n",
      "   rank_test_score  \n",
      "3                1  \n",
      "9                2  \n",
      "1                3  \n",
      "2                3  \n",
      "4                3  \n",
      "TFIDF Vectorizer\n",
      "   param_max_depth param_n_estimators  std_test_score  mean_test_score  \\\n",
      "5               60                300        0.005996         0.747896   \n",
      "8               90                300        0.007018         0.747896   \n",
      "11            None                300        0.008243         0.747666   \n",
      "7               90                150        0.006341         0.743862   \n",
      "2               30                300        0.005828         0.743170   \n",
      "\n",
      "    rank_test_score  \n",
      "5                 1  \n",
      "8                 1  \n",
      "11                3  \n",
      "7                 4  \n",
      "2                 5  \n",
      "Count Vectorizer\n",
      "   param_max_depth param_n_estimators  std_test_score  mean_test_score  \\\n",
      "8               90                300        0.008031         0.747666   \n",
      "11            None                300        0.014520         0.745591   \n",
      "5               60                300        0.007203         0.744553   \n",
      "4               60                150        0.010211         0.739712   \n",
      "2               30                300        0.007453         0.737752   \n",
      "\n",
      "    rank_test_score  \n",
      "8                 1  \n",
      "11                2  \n",
      "5                 3  \n",
      "4                 4  \n",
      "2                 5  \n",
      "TFIDF Vectorizer\n",
      "   param_max_depth param_n_estimators  std_test_score  mean_test_score  \\\n",
      "10            None                150        0.004739         0.621902   \n",
      "7               90                150        0.002323         0.618790   \n",
      "4               60                150        0.002536         0.617983   \n",
      "11            None                300        0.003102         0.617406   \n",
      "8               90                300        0.002856         0.616830   \n",
      "\n",
      "    rank_test_score  \n",
      "10                1  \n",
      "7                 2  \n",
      "4                 3  \n",
      "11                4  \n",
      "8                 5  \n",
      "Count Vectorizer\n",
      "   param_max_depth param_n_estimators  std_test_score  mean_test_score  \\\n",
      "10            None                150        0.003470         0.626282   \n",
      "7               90                150        0.003019         0.622709   \n",
      "4               60                150        0.003059         0.619481   \n",
      "8               90                300        0.002713         0.618329   \n",
      "11            None                300        0.004488         0.617061   \n",
      "\n",
      "    rank_test_score  \n",
      "10                1  \n",
      "7                 2  \n",
      "4                 3  \n",
      "8                 4  \n",
      "11                5  \n",
      "TFIDF Vectorizer\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "\n",
    "def RF_Evaluation(param, X_Features, dataset_PD, lable, is_print = True):\n",
    "    rf = RandomForestClassifier()\n",
    "    gs = GridSearchCV(rf, param, cv=5, n_jobs=-1)#cv=5 means 5 folde validation\n",
    "    gs_fit = gs.fit(X_Features, dataset_PD)\n",
    "    if is_print:\n",
    "        print(pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending = False)[['param_max_depth',\n",
    "        'param_n_estimators', 'std_test_score', 'mean_test_score', 'rank_test_score']][0:5])\n",
    "\n",
    "    return(pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending = False)[['param_max_depth',\n",
    "    'param_n_estimators', 'std_test_score', 'mean_test_score', 'rank_test_score']][0:5])\n",
    "\n",
    "\n",
    "param = {'n_estimators' : [10, 150, 300],\n",
    "        'max_depth' : [30, 60, 90, None]}\n",
    "\n",
    "classes = ['IE' , 'NS', 'FT', 'PJ']\n",
    "for item in classes:\n",
    "    for key, X_Features in features_Dic.items():\n",
    "        RF_Evaluation(param, X_Features, mbti_Dataset[item], item)\n",
    "        print(key)\n"
   ]
  },
  {
   "source": [
    "## 5-2: Gradient Boosting Model\n",
    "### 5-2-1: Gradient Boosting with Holdout test set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ving to 0.1 as Learning_Rate at:  2021-06-26 02:08:21.947917\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 02:20:05.114699\n",
      "Moving to 11 as Max_Depth at:  2021-06-26 02:20:05.114699\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 02:48:08.762527\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 03:05:06.819204\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 03:20:12.301838\n",
      "Moving to 15 as Max_Depth at:  2021-06-26 03:20:12.301838\n",
      "Moving to 150 estimators at:  2021-06-26 03:20:12.301838\n",
      "Moving to TFIDF Vectorizer feature at:  2021-06-26 03:20:12.301838\n",
      "Moving to IE class at:  2021-06-26 03:20:12.301838\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 03:21:31.333904\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 03:22:50.244996\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 03:24:07.262245\n",
      "Moving to 3 as Max_Depth at:  2021-06-26 03:24:07.262245\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 03:26:22.749736\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 03:28:23.732490\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 03:30:00.448052\n",
      "Moving to 7 as Max_Depth at:  2021-06-26 03:30:00.449047\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 03:33:50.473853\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 03:36:45.758365\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 03:38:45.028588\n",
      "Moving to 11 as Max_Depth at:  2021-06-26 03:38:45.028588\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 03:43:50.305800\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 03:47:28.233304\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 03:49:52.332168\n",
      "Moving to 15 as Max_Depth at:  2021-06-26 03:49:52.333165\n",
      "Moving to 50 estimators at:  2021-06-26 03:49:52.333165\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 03:52:29.804288\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 03:55:05.924021\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 03:57:37.383137\n",
      "Moving to 3 as Max_Depth at:  2021-06-26 03:57:37.384134\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 04:02:20.300061\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 04:06:01.956740\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 04:09:12.953989\n",
      "Moving to 7 as Max_Depth at:  2021-06-26 04:09:12.953989\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 04:17:33.697503\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 04:22:33.868668\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 04:26:38.636354\n",
      "Moving to 11 as Max_Depth at:  2021-06-26 04:26:38.636354\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 04:37:05.623742\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 04:42:59.283777\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 04:47:48.993699\n",
      "Moving to 15 as Max_Depth at:  2021-06-26 04:47:48.994696\n",
      "Moving to 100 estimators at:  2021-06-26 04:47:48.994696\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 04:51:46.302980\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 04:55:39.100799\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 04:59:26.428853\n",
      "Moving to 3 as Max_Depth at:  2021-06-26 04:59:26.429850\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 05:06:25.558375\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 05:11:43.282029\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 05:16:31.109808\n",
      "Moving to 7 as Max_Depth at:  2021-06-26 05:16:31.109808\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 05:28:15.112238\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 05:35:09.545842\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 05:41:02.050075\n",
      "Moving to 11 as Max_Depth at:  2021-06-26 05:41:02.051073\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 05:55:46.444325\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 06:03:57.122724\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 06:10:56.815422\n",
      "Moving to 15 as Max_Depth at:  2021-06-26 06:10:56.815422\n",
      "Moving to 150 estimators at:  2021-06-26 06:10:56.815422\n",
      "Moving to Count Vectorizer feature at:  2021-06-26 06:10:56.815422\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 06:12:44.753933\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 06:14:33.434765\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 06:16:20.470688\n",
      "Moving to 3 as Max_Depth at:  2021-06-26 06:16:20.471685\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 06:19:42.298257\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 06:22:48.668038\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 06:25:35.987185\n",
      "Moving to 7 as Max_Depth at:  2021-06-26 06:25:35.987185\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 06:30:51.988379\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 06:35:23.756177\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 06:39:17.778804\n",
      "Moving to 11 as Max_Depth at:  2021-06-26 06:39:17.778804\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 06:45:59.184184\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 06:51:59.206050\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 06:56:54.534003\n",
      "Moving to 15 as Max_Depth at:  2021-06-26 06:56:54.534003\n",
      "Moving to 50 estimators at:  2021-06-26 06:56:54.534003\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 07:00:28.828878\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 07:04:02.816953\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 07:07:35.201468\n",
      "Moving to 3 as Max_Depth at:  2021-06-26 07:07:35.201468\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 07:14:11.320493\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 07:20:07.725923\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 07:25:42.267529\n",
      "Moving to 7 as Max_Depth at:  2021-06-26 07:25:42.267529\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 07:36:50.016381\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 07:45:18.671032\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 07:53:01.647994\n",
      "Moving to 11 as Max_Depth at:  2021-06-26 07:53:01.647994\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 08:07:46.845168\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 08:18:38.871270\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 08:28:27.889842\n",
      "Moving to 15 as Max_Depth at:  2021-06-26 08:28:27.889842\n",
      "Moving to 100 estimators at:  2021-06-26 08:28:27.889842\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 08:33:48.644792\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 08:39:06.792302\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 08:44:26.579921\n",
      "Moving to 3 as Max_Depth at:  2021-06-26 08:44:26.579921\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 08:54:32.880309\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 09:03:17.019448\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 09:11:38.758482\n",
      "Moving to 7 as Max_Depth at:  2021-06-26 09:11:38.758482\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 09:28:05.605551\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 09:40:17.836248\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 09:51:46.259296\n",
      "Moving to 11 as Max_Depth at:  2021-06-26 09:51:46.259296\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 10:13:10.751827\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 10:29:08.162157\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 10:43:49.311148\n",
      "Moving to 15 as Max_Depth at:  2021-06-26 10:43:49.312144\n",
      "Moving to 150 estimators at:  2021-06-26 10:43:49.312144\n",
      "Moving to TFIDF Vectorizer feature at:  2021-06-26 10:43:49.312144\n",
      "Moving to NS class at:  2021-06-26 10:43:49.312144\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 10:45:13.017483\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 10:46:34.611404\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 10:47:53.508533\n",
      "Moving to 3 as Max_Depth at:  2021-06-26 10:47:53.508533\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 10:51:07.544511\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 10:53:55.885991\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 10:55:46.192171\n",
      "Moving to 7 as Max_Depth at:  2021-06-26 10:55:46.193168\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 11:03:02.007443\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 11:09:04.732219\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 11:11:34.099802\n",
      "Moving to 11 as Max_Depth at:  2021-06-26 11:11:34.099802\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 11:23:31.529986\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 11:34:03.985796\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 11:37:30.986138\n",
      "Moving to 15 as Max_Depth at:  2021-06-26 11:37:30.987136\n",
      "Moving to 50 estimators at:  2021-06-26 11:37:30.988135\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 11:40:20.602588\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 11:43:04.268552\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 11:45:42.799965\n",
      "Moving to 3 as Max_Depth at:  2021-06-26 11:45:42.799965\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 11:52:20.726181\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 11:57:49.917979\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 12:01:31.526928\n",
      "Moving to 7 as Max_Depth at:  2021-06-26 12:01:31.527924\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 12:17:10.118650\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 12:27:23.539293\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 12:31:59.824135\n",
      "Moving to 11 as Max_Depth at:  2021-06-26 12:31:59.824135\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 12:55:02.410142\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 13:12:26.170247\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 13:18:30.793881\n",
      "Moving to 15 as Max_Depth at:  2021-06-26 13:18:30.793881\n",
      "Moving to 100 estimators at:  2021-06-26 13:18:30.793881\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 13:22:37.676464\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 13:26:38.513820\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 13:30:30.238705\n",
      "Moving to 3 as Max_Depth at:  2021-06-26 13:30:30.239703\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 13:39:43.785310\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 13:46:54.495229\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 13:52:05.521196\n",
      "Moving to 7 as Max_Depth at:  2021-06-26 13:52:05.521196\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 14:14:44.313223\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 14:29:06.063304\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 14:35:50.794429\n",
      "Moving to 11 as Max_Depth at:  2021-06-26 14:35:50.794429\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 15:12:52.510261\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 15:39:00.562661\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 15:47:57.792875\n",
      "Moving to 15 as Max_Depth at:  2021-06-26 15:47:57.792875\n",
      "Moving to 150 estimators at:  2021-06-26 15:47:57.792875\n",
      "Moving to Count Vectorizer feature at:  2021-06-26 15:47:57.792875\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 15:49:48.586018\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 15:51:38.150429\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 15:53:25.730187\n",
      "Moving to 3 as Max_Depth at:  2021-06-26 15:53:25.731183\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 15:57:39.245845\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 16:01:24.689488\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 16:04:23.528753\n",
      "Moving to 7 as Max_Depth at:  2021-06-26 16:04:23.528753\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 16:12:33.368935\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 16:19:57.646448\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 16:24:16.319366\n",
      "Moving to 11 as Max_Depth at:  2021-06-26 16:24:16.319366\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 16:36:27.087407\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 16:48:49.776410\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 16:54:34.120899\n",
      "Moving to 15 as Max_Depth at:  2021-06-26 16:54:34.120899\n",
      "Moving to 50 estimators at:  2021-06-26 16:54:34.120899\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 16:58:16.150656\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 17:01:52.453033\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 17:05:26.007262\n",
      "Moving to 3 as Max_Depth at:  2021-06-26 17:05:26.007262\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 17:13:41.325934\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 17:20:53.998576\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 17:26:44.930842\n",
      "Moving to 7 as Max_Depth at:  2021-06-26 17:26:44.930842\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 17:44:00.197233\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 17:57:08.187088\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 18:05:25.453441\n",
      "Moving to 11 as Max_Depth at:  2021-06-26 18:05:25.453441\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 18:32:45.743299\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 18:53:40.471624\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 19:04:49.956411\n",
      "Moving to 15 as Max_Depth at:  2021-06-26 19:04:49.957407\n",
      "Moving to 100 estimators at:  2021-06-26 19:04:49.957407\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 19:10:19.810039\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 19:15:45.275080\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 19:21:04.102939\n",
      "Moving to 3 as Max_Depth at:  2021-06-26 19:21:04.102939\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 19:33:07.388788\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 19:43:13.794858\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 19:51:54.650030\n",
      "Moving to 7 as Max_Depth at:  2021-06-26 19:51:54.651027\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 20:17:58.341014\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 20:36:36.448921\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 20:49:04.176330\n",
      "Moving to 11 as Max_Depth at:  2021-06-26 20:49:04.176330\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 21:30:24.394077\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 22:03:42.117592\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 22:20:11.184244\n",
      "Moving to 15 as Max_Depth at:  2021-06-26 22:20:11.184244\n",
      "Moving to 150 estimators at:  2021-06-26 22:20:11.184244\n",
      "Moving to TFIDF Vectorizer feature at:  2021-06-26 22:20:11.184244\n",
      "Moving to FT class at:  2021-06-26 22:20:11.184244\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 22:21:32.105765\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 22:22:52.287583\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 22:24:10.061423\n",
      "Moving to 3 as Max_Depth at:  2021-06-26 22:24:10.061423\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 22:27:09.219417\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 22:29:37.102510\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 22:31:24.894097\n",
      "Moving to 7 as Max_Depth at:  2021-06-26 22:31:24.895093\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 22:37:58.884227\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 22:42:39.174857\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 22:44:53.969790\n",
      "Moving to 11 as Max_Depth at:  2021-06-26 22:44:53.970786\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 22:53:39.614462\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 23:00:44.904535\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 23:03:36.288325\n",
      "Moving to 15 as Max_Depth at:  2021-06-26 23:03:36.288325\n",
      "Moving to 50 estimators at:  2021-06-26 23:03:36.288325\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 23:06:17.303402\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 23:08:55.395379\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 23:11:30.062826\n",
      "Moving to 3 as Max_Depth at:  2021-06-26 23:11:30.062826\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 23:17:28.994752\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 23:21:54.574055\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 23:25:19.858977\n",
      "Moving to 7 as Max_Depth at:  2021-06-26 23:25:19.859974\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-26 23:38:21.387164\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-26 23:45:30.652096\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-26 23:49:52.017538\n",
      "Moving to 11 as Max_Depth at:  2021-06-26 23:49:52.017538\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-27 00:09:17.649060\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-27 00:19:18.851078\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-27 00:24:47.493756\n",
      "Moving to 15 as Max_Depth at:  2021-06-27 00:24:47.494753\n",
      "Moving to 100 estimators at:  2021-06-27 00:24:47.494753\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-27 00:28:48.572575\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-27 00:32:44.595558\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-27 00:36:36.735110\n",
      "Moving to 3 as Max_Depth at:  2021-06-27 00:36:36.735110\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-27 00:45:24.912384\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-27 00:51:35.895832\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-27 00:56:38.713192\n",
      "Moving to 7 as Max_Depth at:  2021-06-27 00:56:38.713192\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-27 01:16:16.315010\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-27 01:26:09.929221\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-27 01:33:21.430721\n",
      "Moving to 11 as Max_Depth at:  2021-06-27 01:33:21.430721\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-27 02:09:21.425489\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-27 02:22:48.952864\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-27 02:30:49.723039\n",
      "Moving to 15 as Max_Depth at:  2021-06-27 02:30:49.723039\n",
      "Moving to 150 estimators at:  2021-06-27 02:30:49.724039\n",
      "Moving to Count Vectorizer feature at:  2021-06-27 02:30:49.724039\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-27 02:32:39.360637\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-27 02:34:27.325125\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-27 02:36:14.956226\n",
      "Moving to 3 as Max_Depth at:  2021-06-27 02:36:14.957223\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-27 02:40:10.061837\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-27 02:43:35.782532\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-27 02:46:32.618197\n",
      "Moving to 7 as Max_Depth at:  2021-06-27 02:46:32.618197\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-27 02:53:45.750675\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-27 02:59:59.008036\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-27 03:04:06.070567\n",
      "Moving to 11 as Max_Depth at:  2021-06-27 03:04:06.071565\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-27 03:14:29.730531\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-27 03:23:25.940862\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-27 03:28:44.244062\n",
      "Moving to 15 as Max_Depth at:  2021-06-27 03:28:44.244062\n",
      "Moving to 50 estimators at:  2021-06-27 03:28:44.244062\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-27 03:32:20.642468\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-27 03:35:57.282337\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-27 03:39:31.914048\n",
      "Moving to 3 as Max_Depth at:  2021-06-27 03:39:31.915045\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-27 03:47:26.110710\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-27 03:53:56.136178\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-27 03:59:50.802855\n",
      "Moving to 7 as Max_Depth at:  2021-06-27 03:59:50.802855\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-27 04:15:32.871845\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-27 04:25:58.432795\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-27 04:34:19.027493\n",
      "Moving to 11 as Max_Depth at:  2021-06-27 04:34:19.028492\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-27 04:57:08.104573\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-27 05:11:56.601331\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-27 05:22:29.254491\n",
      "Moving to 15 as Max_Depth at:  2021-06-27 05:22:29.254491\n",
      "Moving to 100 estimators at:  2021-06-27 05:22:29.254491\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-27 05:28:01.005869\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-27 05:33:28.941562\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-27 05:38:53.151620\n",
      "Moving to 3 as Max_Depth at:  2021-06-27 05:38:53.151620\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-27 05:50:29.133430\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-27 06:00:01.406299\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-27 06:08:35.587730\n",
      "Moving to 7 as Max_Depth at:  2021-06-27 06:08:35.588728\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-27 06:32:38.453093\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-27 06:47:11.646134\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-27 06:59:20.278071\n",
      "Moving to 11 as Max_Depth at:  2021-06-27 06:59:20.278071\n",
      "Moving to 0.01 as Learning_Rate at:  2021-06-27 07:35:38.691122\n",
      "Moving to 0.1 as Learning_Rate at:  2021-06-27 07:55:51.517885\n",
      "Moving to 1.0 as Learning_Rate at:  2021-06-27 08:11:27.720875\n",
      "Moving to 15 as Max_Depth at:  2021-06-27 08:11:27.720875\n",
      "Moving to 150 estimators at:  2021-06-27 08:11:27.720875\n",
      "Moving to TFIDF Vectorizer feature at:  2021-06-27 08:11:27.720875\n",
      "Moving to PJ class at:  2021-06-27 08:11:27.720875\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         Type            Method  Estimators  Max_Depth  Learning_Rate  \\\n",
       "0  Introverts  Count Vectorizer          50          3           0.01   \n",
       "1  Introverts  Count Vectorizer          50          3           0.10   \n",
       "2  Introverts  Count Vectorizer          50          3           1.00   \n",
       "3  Introverts  Count Vectorizer          50          7           0.01   \n",
       "4  Introverts  Count Vectorizer          50          7           0.10   \n",
       "\n",
       "   Precision    Recall  Accuracy  \n",
       "0   0.774640  1.000000     0.775  \n",
       "1   0.828499  0.973333     0.822  \n",
       "2   0.842327  0.891030     0.786  \n",
       "3   0.758083  0.999239     0.758  \n",
       "4   0.821705  0.960725     0.811  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Type</th>\n      <th>Method</th>\n      <th>Estimators</th>\n      <th>Max_Depth</th>\n      <th>Learning_Rate</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Introverts</td>\n      <td>Count Vectorizer</td>\n      <td>50</td>\n      <td>3</td>\n      <td>0.01</td>\n      <td>0.774640</td>\n      <td>1.000000</td>\n      <td>0.775</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Introverts</td>\n      <td>Count Vectorizer</td>\n      <td>50</td>\n      <td>3</td>\n      <td>0.10</td>\n      <td>0.828499</td>\n      <td>0.973333</td>\n      <td>0.822</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Introverts</td>\n      <td>Count Vectorizer</td>\n      <td>50</td>\n      <td>3</td>\n      <td>1.00</td>\n      <td>0.842327</td>\n      <td>0.891030</td>\n      <td>0.786</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Introverts</td>\n      <td>Count Vectorizer</td>\n      <td>50</td>\n      <td>7</td>\n      <td>0.01</td>\n      <td>0.758083</td>\n      <td>0.999239</td>\n      <td>0.758</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Introverts</td>\n      <td>Count Vectorizer</td>\n      <td>50</td>\n      <td>7</td>\n      <td>0.10</td>\n      <td>0.821705</td>\n      <td>0.960725</td>\n      <td>0.811</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None, 'display.max_colwidth', 500)\n",
    "\n",
    "\n",
    "def GBoosting_GridSearch(X_Features, dataset_PD, test_Szie, Lable, feature_type , n_est, depth, lr ,is_print = True):\n",
    "    if Lable == 'IE':\n",
    "        Lable = 'I'\n",
    "        predict_Lable = 'Introverts'\n",
    "    elif Lable == 'NS':\n",
    "        Lable = 'N'\n",
    "        predict_Lable = 'Intuitives'\n",
    "    elif Lable == 'FT':\n",
    "        Lable = 'F'\n",
    "        predict_Lable = 'Feelers'\n",
    "    elif Lable == 'PJ':\n",
    "        Lable = 'P'\n",
    "        predict_Lable = 'Perceivers'\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_Features, dataset_PD, test_size=test_Szie) # 20% of our dataset is test set\n",
    "    gb = GradientBoostingClassifier(n_estimators=n_est, max_depth=depth , learning_rate=lr)#Max depth of tree is 20\n",
    "    gb_model = gb.fit(X_train, Y_train)\n",
    "    Y_pred = gb_model.predict(X_test)\n",
    "    precision, recall, fscore, support = score(Y_test, Y_pred, pos_label=Lable, average='binary')\n",
    "    if is_print:\n",
    "        print('Being ' + predict_Lable + ' using ' + feature_type + ': '  ,'Estimators: {} / Max_Depth: {} / Learning_Rate: {} --> Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "                                                        n_est,\n",
    "                                                        depth,\n",
    "                                                        lr,\n",
    "                                                        round(precision, 3),\n",
    "                                                        round(recall, 3), \n",
    "                                                        round((Y_pred==Y_test).sum() / len(Y_pred),3)))\n",
    "    return([predict_Lable, feature_type, n_est, depth, lr, precision, recall, round((Y_pred==Y_test).sum() / len(Y_pred),3)])\n",
    "\n",
    "\n",
    "\n",
    "classes = ['IE' , 'NS', 'FT', 'PJ']\n",
    "gb_result = []\n",
    "\n",
    "for item in classes:\n",
    "    for key, X_Features in features_Dic.items():\n",
    "        for n_est in [50, 100, 150]:\n",
    "            for depth in [3, 7, 11, 15]:\n",
    "                for lr in [0.01, 0.10, 1.00]:\n",
    "                    gb_result.append(GBoosting_GridSearch(X_Features, mbti_Dataset[item], 0.2, item, key, n_est, depth, lr, is_print=False))\n",
    "                    print('Moving to {} as Learning_Rate at: '.format(lr), datetime.now())\n",
    "                print('Moving to {} as Max_Depth at: '.format(depth), datetime.now())\n",
    "            print('Moving to {} estimators at: '.format(n_est), datetime.now())\n",
    "        print('Moving to {} feature at: '.format(key), datetime.now())\n",
    "    print('Moving to {} class at: '.format(item), datetime.now())\n",
    "\n",
    "pd.DataFrame(gb_result, columns= ['Type', 'Method', 'Estimators', 'Max_Depth', 'Learning_Rate','Precision', 'Recall', 'Accuracy']).to_csv('GB_Holdout_Result.csv')\n",
    "pd.DataFrame(gb_result, columns= ['Type', 'Method', 'Estimators', 'Max_Depth', 'Learning_Rate','Precision', 'Recall', 'Accuracy']).head()\n"
   ]
  },
  {
   "source": [
    "### 5-2-2: Evaluation Gradient Boosting Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "\n",
    "\n",
    "def GB_Evaluation(param, X_Features, dataset_PD, lable):\n",
    "    gb = GradientBoostingClassifier()\n",
    "    gs = GridSearchCV(gb, param, cv=5)#cv=5 meand 5 folde validation\n",
    "    gs_fit = gs.fit(X_Features, dataset_PD)\n",
    "    print(pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending = False)[['param_max_depth',\n",
    "     'param_n_estimators', 'std_test_score', 'mean_test_score', 'rank_test_score']][0:5])\n",
    "\n",
    "    return(pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending = False)[['param_max_depth',\n",
    "     'param_n_estimators', 'std_test_score', 'mean_test_score', 'rank_test_score']][0:5])\n",
    "  \n",
    "param = {'n_estimators' : [50, 100, 150],\n",
    "                'max_depth' : [7, 11, 15],\n",
    "                'Learning_rate' : [0.1]}\n",
    "\n",
    "classes = ['IE' , 'NS', 'FT', 'PJ']\n",
    "\n",
    "\n",
    "for item in classes:\n",
    "    for key, X_Features in features_Dic.items():\n",
    "        GB_Evaluation(param, X_Features, mbti_Dataset[item], item)\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "frequency = 2500  # Set Frequency To 2500 Hertz\n",
    "duration = 1000  # Set Duration To 1000 ms == 1 second\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}