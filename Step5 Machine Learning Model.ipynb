{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setep 5: Machine Learning Model\n",
    "\n",
    "<br>\n",
    "Clean up any values left from any previous steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick clean up\n",
    "def cleanup():\n",
    "    for name in dir():\n",
    "        if not name.startswith('_'):\n",
    "            del globals()[name]\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>IE</th>\n",
       "      <th>NS</th>\n",
       "      <th>FT</th>\n",
       "      <th>PJ</th>\n",
       "      <th>Posts_Lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>J</td>\n",
       "      <td>[, moment, sportscenter, top, ten, play, prank, lifechanging, experience, life, repeat, today, may, perc, experience, immerse, last, thing, friend, posted, facebook, committing, suicide, next, day, rest, peace, hello, sorry, hear, distress, natural, relationship, perfection, time, every, moment, existence, try, figure, hard, time, time, growth, welcome, stuff, game, set, match, prozac, wellbrutin, least, thirty, minute, moving, leg, mean, moving, sitting, desk, chair, weed, moderation, maybe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>E</td>\n",
       "      <td>N</td>\n",
       "      <td>T</td>\n",
       "      <td>P</td>\n",
       "      <td>[finding, lack, post, alarming, sex, boring, position, often, example, girlfriend, currently, environment, creatively, use, cowgirl, missionary, enough, giving, new, meaning, game, theory, hello, grin, take, converse, flirting, acknowledge, presence, return, word, smooth, wordplay, cheeky, grin, lack, balance, hand, eye, coordination, real, iq, test, score, internet, iq, test, funny, score, higher, like, former, response, thread, mention, believe, iq, test, banish, know, vanish, site, year, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>T</td>\n",
       "      <td>P</td>\n",
       "      <td>[good, one, course, say, know, blessing, curse, absolutely, positive, best, friend, could, amazing, couple, count, yes, could, madly, love, case, reconciled, feeling, thank, link, socalled, tisi, loop, stem, current, topicobsession, deadly, like, stuck, thought, mind, wanders, circle, feel, truly, terrible, noticed, peculiar, vegetation, look, grass, dozen, different, plant, specie, imagine, hundred, year, later, whenif, soil, smith, â, never, one, ever, often, find, spotting, face, marble, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>I</td>\n",
       "      <td>N</td>\n",
       "      <td>T</td>\n",
       "      <td>J</td>\n",
       "      <td>[dear, enjoyed, conversation, day, esoteric, gabbing, nature, universe, idea, every, rule, social, code, arbitrary, construct, created, dear, sub, long, time, see, sincerely, alpha, none, type, hurt, deep, existential, way, want, part, probably, sliding, scale, depends, individual, preference, like, everything, humanity, draco, malfoy, also, would, say, either, either, though, stacking, somewhat, arbitrary, distinction, make, believe, core, indicates, primary, motivation, hand, every, action...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>E</td>\n",
       "      <td>N</td>\n",
       "      <td>T</td>\n",
       "      <td>J</td>\n",
       "      <td>[fired, another, silly, misconception, approaching, logically, going, key, unlocking, whatever, think, entitled, nobody, want, approached, b, guy, really, want, go, superduperlongass, vacation, cmon, guy, bos, listen, get, even, approached, logically, everything, never, mind, go, permanent, vacation, two, month, would, crazy, idea, really, best, employee, may, cooking, want, reliable, asset, gone, long, employer, lol, like, view, unsolicited, victim, sometimes, really, like, impoverished, ra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type IE NS FT PJ  \\\n",
       "0  INFJ  I  N  F  J   \n",
       "1  ENTP  E  N  T  P   \n",
       "2  INTP  I  N  T  P   \n",
       "3  INTJ  I  N  T  J   \n",
       "4  ENTJ  E  N  T  J   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Posts_Lemmatized  \n",
       "0  [, moment, sportscenter, top, ten, play, prank, lifechanging, experience, life, repeat, today, may, perc, experience, immerse, last, thing, friend, posted, facebook, committing, suicide, next, day, rest, peace, hello, sorry, hear, distress, natural, relationship, perfection, time, every, moment, existence, try, figure, hard, time, time, growth, welcome, stuff, game, set, match, prozac, wellbrutin, least, thirty, minute, moving, leg, mean, moving, sitting, desk, chair, weed, moderation, maybe...  \n",
       "1  [finding, lack, post, alarming, sex, boring, position, often, example, girlfriend, currently, environment, creatively, use, cowgirl, missionary, enough, giving, new, meaning, game, theory, hello, grin, take, converse, flirting, acknowledge, presence, return, word, smooth, wordplay, cheeky, grin, lack, balance, hand, eye, coordination, real, iq, test, score, internet, iq, test, funny, score, higher, like, former, response, thread, mention, believe, iq, test, banish, know, vanish, site, year, ...  \n",
       "2  [good, one, course, say, know, blessing, curse, absolutely, positive, best, friend, could, amazing, couple, count, yes, could, madly, love, case, reconciled, feeling, thank, link, socalled, tisi, loop, stem, current, topicobsession, deadly, like, stuck, thought, mind, wanders, circle, feel, truly, terrible, noticed, peculiar, vegetation, look, grass, dozen, different, plant, specie, imagine, hundred, year, later, whenif, soil, smith, â, never, one, ever, often, find, spotting, face, marble, ...  \n",
       "3  [dear, enjoyed, conversation, day, esoteric, gabbing, nature, universe, idea, every, rule, social, code, arbitrary, construct, created, dear, sub, long, time, see, sincerely, alpha, none, type, hurt, deep, existential, way, want, part, probably, sliding, scale, depends, individual, preference, like, everything, humanity, draco, malfoy, also, would, say, either, either, though, stacking, somewhat, arbitrary, distinction, make, believe, core, indicates, primary, motivation, hand, every, action...  \n",
       "4  [fired, another, silly, misconception, approaching, logically, going, key, unlocking, whatever, think, entitled, nobody, want, approached, b, guy, really, want, go, superduperlongass, vacation, cmon, guy, bos, listen, get, even, approached, logically, everything, never, mind, go, permanent, vacation, two, month, would, crazy, idea, really, best, employee, may, cooking, want, reliable, asset, gone, long, employer, lol, like, view, unsolicited, victim, sometimes, really, like, impoverished, ra...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load information from prevous steps\n",
    "import pandas as pd\n",
    "\n",
    "fields = ['Type', 'IE', 'NS', 'FT', 'PJ', 'Posts_Lemmatized']\n",
    "mbti_Dataset = pd.read_pickle('mbti_Dataset2.pkl')\n",
    "mbti_Dataset = mbti_Dataset.filter(fields)\n",
    "mbti_Dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-1: Split our data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(mbti_Dataset['Posts_Lemmatized'], mbti_Dataset[['IE' , 'NS', 'FT', 'PJ']], test_size=0.2)#20% of dataset is test set    \r\n",
    "\r\n",
    "\r\n",
    "X_train.to_pickle('X_train.pkl')\r\n",
    "X_test.to_pickle('X_test.pkl')\r\n",
    "Y_train.to_pickle('Y_train.pkl')\r\n",
    "Y_test.to_pickle('Y_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "import numpy as np\r\n",
    "from scipy import sparse\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "def lemmatizing(text):\r\n",
    "    \"\"\"Lemmatizing the input text using WordNet and NLTK package\"\"\"\r\n",
    "    NLTK_WNL = nltk.WordNetLemmatizer()\r\n",
    "    text_Lem = [NLTK_WNL.lemmatize(word) for word in text]\r\n",
    "    return(text_Lem)\r\n",
    "\r\n",
    "\r\n",
    "def clean_and_load():\r\n",
    "    for name in dir():\r\n",
    "        if not name.startswith('_'):\r\n",
    "            del globals()[name]\r\n",
    "    global X_train, X_test, Y_train, Y_test\r\n",
    "    X_train = pd.read_pickle('X_train.pkl')\r\n",
    "    X_test = pd.read_pickle('X_test.pkl')\r\n",
    "    Y_train = pd.read_pickle('Y_train.pkl')\r\n",
    "    Y_test = pd.read_pickle('Y_test.pkl')\r\n",
    "\r\n",
    "\r\n",
    "#TFIDF\r\n",
    "clean_and_load()\r\n",
    "tfidf_vect = TfidfVectorizer(analyzer=lemmatizing)\r\n",
    "tfidf_vect_fit = tfidf_vect.fit(X_train)\r\n",
    "\r\n",
    "tfidf_train = tfidf_vect_fit.transform(X_train)\r\n",
    "tfidf_test = tfidf_vect_fit.transform(X_test)\r\n",
    "\r\n",
    "X_train_tfidf = pd.DataFrame(tfidf_train.toarray())\r\n",
    "X_test_tfidf = pd.DataFrame(tfidf_test.toarray())\r\n",
    "\r\n",
    "np.save('X_train_tfidf', X_train_tfidf)#Saving results\r\n",
    "np.save('X_test_tfidf', X_test_tfidf)#Saving results\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "#CountVectorizer\r\n",
    "clean_and_load()\r\n",
    "count_vect = CountVectorizer(analyzer=lemmatizing)\r\n",
    "count_vect_fit = count_vect.fit(X_train)\r\n",
    "\r\n",
    "count_train = count_vect_fit.transform(X_train)\r\n",
    "count_test = count_vect_fit.transform(X_test)\r\n",
    "\r\n",
    "X_train_count = pd.DataFrame(count_train.toarray())\r\n",
    "X_test_count = pd.DataFrame(count_test.toarray())\r\n",
    "\r\n",
    "np.save('X_train_count', X_train_count)#Saving results\r\n",
    "np.save('X_test_count', X_test_count)#Saving results\r\n",
    "\r\n",
    "\r\n",
    "#N-gram\r\n",
    "#clean_and_load()\r\n",
    "#ngram_vect = CountVectorizer(ngram_range=(1,3)) #Running N-Gram on full dataset (Max = 3)\r\n",
    "#X_train = X_train.apply(lambda x: ' '.join(map(str,x)))\r\n",
    "#X_test = X_test.apply(lambda x: ' '.join(map(str,x)))\r\n",
    "#ngram_vect_fit = ngram_vect.fit(X_train)\r\n",
    "\r\n",
    "#ngram_train = ngram_vect_fit.transform(X_train)\r\n",
    "#ngram_test = ngram_vect_fit.transform(X_test)\r\n",
    "\r\n",
    "#X_train_ngram = pd.DataFrame(ngram_train.toarray())\r\n",
    "#X_test_ngram = pd.DataFrame(ngram_test.toarray())\r\n",
    "\r\n",
    "#np.save('X_train_ngram', X_train_ngram)#Saving results\r\n",
    "#np.save('D:\\X_test_ngram', X_test_ngram)#Saving results\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-1: Random Forest Model\n",
    "### 5-1-1: Explorering Random Forest with Holdout test set + grid-search\n",
    "#### 5-1-1-1 : Split data into training and test set\n",
    "#### 5-1-1-2 : Train vectorizes on training set and use that to transform test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Being Introverts using TFIDF: Estimators: 10 / Max_Depth: 10 --> Precision: 0.753 / Recall: 1.0 / Accuracy: 0.753\n",
      "Being Introverts using TFIDF: Estimators: 10 / Max_Depth: 20 --> Precision: 0.753 / Recall: 0.999 / Accuracy: 0.752\n",
      "Being Introverts using TFIDF: Estimators: 10 / Max_Depth: 30 --> Precision: 0.754 / Recall: 0.996 / Accuracy: 0.752\n",
      "Being Introverts using TFIDF: Estimators: 10 / Max_Depth: None --> Precision: 0.758 / Recall: 0.961 / Accuracy: 0.74\n",
      "Being Introverts using TFIDF: Estimators: 50 / Max_Depth: 10 --> Precision: 0.753 / Recall: 1.0 / Accuracy: 0.753\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-bd80f6910a80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mn_est\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdepth\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m                 \u001b[0mrf_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrfClassifier_GridSearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_est\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_est\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mis_print\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-bd80f6910a80>\u001b[0m in \u001b[0;36mrfClassifier_GridSearch\u001b[1;34m(X_test_set, X_train_set, Lable, n_est, depth, feature_type, is_print)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mrf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_est\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#Max depth of tree is 20\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mrf_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mLable\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mY_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    301\u001b[0m                 \u001b[1;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m             )\n\u001b[1;32m--> 303\u001b[1;33m         X, y = self._validate_data(X, y, multi_output=True,\n\u001b[0m\u001b[0;32m    304\u001b[0m                                    accept_sparse=\"csc\", dtype=DTYPE)\n\u001b[0;32m    305\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    793\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    796\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    596\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from scipy import sparse\r\n",
    "\r\n",
    "pd.set_option('display.max_rows', None, 'display.max_colwidth', 500)\r\n",
    "\r\n",
    "\r\n",
    "def rfClassifier_GridSearch(X_test_set, X_train_set, Lable, n_est, depth, feature_type, is_print = True):\r\n",
    "    if Lable == 'IE':\r\n",
    "        s_Lable = 'I'\r\n",
    "        predict_Lable = 'Introverts'\r\n",
    "    elif Lable == 'NS':\r\n",
    "        s_Lable = 'N'\r\n",
    "        predict_Lable = 'Intuitives'\r\n",
    "    elif Lable == 'FT':\r\n",
    "        s_Lable = 'F'\r\n",
    "        predict_Lable = 'Feelers'\r\n",
    "    elif Lable == 'PJ':\r\n",
    "        s_Lable = 'P'\r\n",
    "        predict_Lable = 'Perceivers'\r\n",
    "\r\n",
    "    rf = RandomForestClassifier(n_estimators=n_est, max_depth=depth ,n_jobs=-1)#Max depth of tree is 20\r\n",
    "    rf_model = rf.fit(X_train_set, Y_train[Lable])\r\n",
    "    Y_pred = rf_model.predict(X_test_set)\r\n",
    "\r\n",
    "    precision, recall, fscore, support = score(Y_test[Lable], Y_pred, pos_label=s_Lable, average='binary')\r\n",
    "    if is_print:\r\n",
    "        print('Being ' + predict_Lable + ' using ' + feature_type + ':'  ,\r\n",
    "        'Estimators: {} / Max_Depth: {} --> Precision: {} / Recall: {} / Accuracy: {}'.format(\r\n",
    "                                                        n_est,\r\n",
    "                                                        depth,\r\n",
    "                                                        round(precision, 3),\r\n",
    "                                                        round(recall, 3), \r\n",
    "                                                        round((Y_pred==Y_test[Lable]).sum() / len(Y_pred),3)))\r\n",
    "    return([predict_Lable, feature_type, n_est, depth, precision, recall, round((Y_pred==Y_test[Lable]).sum() / len(Y_pred),3)])\r\n",
    "\r\n",
    "\r\n",
    "classes = ['IE' , 'NS', 'FT', 'PJ']\r\n",
    "rf_result = []\r\n",
    "\r\n",
    "Y_train = pd.read_pickle('Y_train.pkl')\r\n",
    "Y_test = pd.read_pickle('Y_test.pkl')\r\n",
    "\r\n",
    "for feature_type in ['TFIDF', 'CountVectorizer', 'N-gram']:\r\n",
    "    if feature_type == 'TFIDF':\r\n",
    "        X_test_set = np.load('X_test_tfidf.npy')\r\n",
    "        X_train_set = np.load('X_train_tfidf.npy')\r\n",
    "    elif feature_type == 'CountVectorizer':\r\n",
    "        X_test_set = np.load('X_test_count.npy')\r\n",
    "        X_train_set = np.load('X_train_count.npy')\r\n",
    "    elif feature_type == 'N-gram':\r\n",
    "        #X_test_set = np.load('X_test_ngram.npy')\r\n",
    "        #X_train_set = np.load('X_train_ngram.npy')\r\n",
    "        print()\r\n",
    "    for item in classes:\r\n",
    "        for n_est in [10, 50, 100]:\r\n",
    "            for depth in [10, 20, 30, None]:\r\n",
    "                rf_result.append(rfClassifier_GridSearch(X_test_set = X_test_set, X_train_set = X_train_set, Lable = item, n_est=n_est, depth=depth, feature_type=feature_type,is_print = True))\r\n",
    "\r\n",
    "\r\n",
    "pd.DataFrame(rf_result, columns= ['Type', 'Method', 'Estimators', 'Max_Depth', 'Precision', 'Recall', 'Accuracy']).to_csv('RF_Holdout_Result.csv')\r\n",
    "pd.DataFrame(rf_result, columns= ['Type', 'Method', 'Estimators', 'Max_Depth', 'Precision', 'Recall', 'Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "import pandas as pd\r\n",
    "#from scipy import sparse\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "pd.set_option('display.max_colwidth', 500)\r\n",
    "\r\n",
    "def RF_Evaluation(param, Lable, is_print = True):\r\n",
    "    rf = RandomForestClassifier()\r\n",
    "    gs = GridSearchCV(rf, param, cv=5)#cv=5 means 5 folde validation\r\n",
    "    gs_fit = gs.fit(X_train_set, Y_train[Lable])\r\n",
    "    \r\n",
    "    if is_print:\r\n",
    "        print(pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending = False)[['param_max_depth',\r\n",
    "        'param_n_estimators', 'std_test_score', 'mean_test_score', 'rank_test_score']][0:5])\r\n",
    "\r\n",
    "    return(pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending = False)[['param_max_depth',\r\n",
    "    'param_n_estimators', 'std_test_score', 'mean_test_score', 'rank_test_score']])\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "param = {'n_estimators' : [10, 150, 300],\r\n",
    "        'max_depth' : [30, 60, 90, None]}\r\n",
    "\r\n",
    "classes = ['IE' , 'NS', 'FT', 'PJ']\r\n",
    "Y_train = pd.read_pickle('Y_train.pkl')\r\n",
    "rf_parm_result = []\r\n",
    "\r\n",
    "\r\n",
    "for feature_type in ['TFIDF', 'CountVectorizer', 'N-gram']:\r\n",
    "    if feature_type == 'TFIDF':\r\n",
    "        X_train_set = np.load('X_train_tfidf.npy')\r\n",
    "    elif feature_type == 'CountVectorizer':\r\n",
    "        X_train_set = np.load('X_train_count.npy')\r\n",
    "#    elif feature_type == 'N-gram':\r\n",
    "#        X_train_set = np.load('X_train_ngram.npy')\r\n",
    "     for item in classes:\r\n",
    "        rf_parm_result.append(RF_Evaluation(param, Lable = item))\r\n",
    "\r\n",
    "pd.DataFrame(rf_parm_result).to_csv('RF_Parm_Result.csv')\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-2: Gradient Boosting Model\n",
    "### 5-2-1: Gradient Boosting with Holdout test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.ensemble import GradientBoostingClassifier\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from datetime import datetime\r\n",
    "\r\n",
    "\r\n",
    "pd.set_option('display.max_rows', None, 'display.max_colwidth', 500)\r\n",
    "\r\n",
    "\r\n",
    "def GBoosting_GridSearch(Lable, n_est, depth, lr, feature_type, is_print = True):\r\n",
    "    if Lable == 'IE':\r\n",
    "        s_Lable = 'I'\r\n",
    "        predict_Lable = 'Introverts'\r\n",
    "    elif Lable == 'NS':\r\n",
    "        s_Lable = 'N'\r\n",
    "        predict_Lable = 'Intuitives'\r\n",
    "    elif Lable == 'FT':\r\n",
    "        s_Lable = 'F'\r\n",
    "        predict_Lable = 'Feelers'\r\n",
    "    elif Lable == 'PJ':\r\n",
    "        s_Lable = 'P'\r\n",
    "        predict_Lable = 'Perceivers'\r\n",
    "\r\n",
    "    gb = GradientBoostingClassifier(n_estimators=n_est, max_depth=depth , learning_rate=lr)\r\n",
    "    gb_model = gb.fit(X_train_set, Y_train[Lable])\r\n",
    "    Y_pred = gb_model.predict(X_test_set)\r\n",
    "\r\n",
    "    precision, recall, fscore, support = score(Y_test[Lable], Y_pred, pos_label=s_Lable, average='binary')\r\n",
    "    if is_print:\r\n",
    "        print('Being ' + predict_Lable + ' using ' + feature_type + ': '  ,'Estimators: {} / Max_Depth: {} / Learning_Rate: {} --> Precision: {} / Recall: {} / Accuracy: {}'.format(\r\n",
    "                                                        n_est,\r\n",
    "                                                        depth,\r\n",
    "                                                        lr,\r\n",
    "                                                        round(precision, 3),\r\n",
    "                                                        round(recall, 3), \r\n",
    "                                                        round((Y_pred==Y_test[Lable]).sum() / len(Y_pred),3)))\r\n",
    "    return([predict_Lable, feature_type, n_est, depth, lr, precision, recall, round((Y_pred==Y_test[Lable]).sum() / len(Y_pred),3)])\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "classes = ['IE' , 'NS', 'FT', 'PJ']\r\n",
    "gb_result = []\r\n",
    "\r\n",
    "Y_train = pd.read_pickle('Y_train.pkl')\r\n",
    "Y_test = pd.read_pickle('Y_test.pkl')\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "for feature_type in ['TFIDF', 'CountVectorizer', 'N-gram']:\r\n",
    "    if feature_type == 'TFIDF':\r\n",
    "        X_test_set = np.load('X_test_tfidf.npy')\r\n",
    "        X_train_set = np.load('X_train_tfidf.npy')\r\n",
    "    elif feature_type == 'CountVectorizer':\r\n",
    "        X_test_set = np.load('X_test_count.npy')\r\n",
    "        X_train_set = np.load('X_train_count.npy')\r\n",
    "    elif feature_type == 'N-gram':\r\n",
    "        #X_test_set = np.load('X_test_ngram.npy')\r\n",
    "        #X_train_set = np.load('X_train_ngram.npy')\r\n",
    "        print()\r\n",
    "    for item in classes:\r\n",
    "        for n_est in [10, 50, 100]:\r\n",
    "            for depth in [10, 20, 30, None]:\r\n",
    "                for lr in [0.01, 0.10, 1.00]:\r\n",
    "                    gb_result.append(GBoosting_GridSearch(Lable = item, n_est=n_est, depth=depth, lr = lr, feature_type=feature_type))\r\n",
    "\r\n",
    "pd.DataFrame(gb_result, columns= ['Type', 'Method', 'Estimators', 'Max_Depth', 'Learning_Rate','Precision', 'Recall', 'Accuracy']).to_csv('GB_Holdout_Result.csv')\r\n",
    "pd.DataFrame(gb_result, columns= ['Type', 'Method', 'Estimators', 'Max_Depth', 'Learning_Rate','Precision', 'Recall', 'Accuracy']).head()\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2-2: Evaluation Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "pd.set_option('display.max_colwidth', 500)\r\n",
    "\r\n",
    "\r\n",
    "def GB_Evaluation(param, Lable, is_print = True):\r\n",
    "    gb = GradientBoostingClassifier()\r\n",
    "    gs = GridSearchCV(gb, param, cv=5)#cv=5 meand 5 folde validation\r\n",
    "    gs_fit = gs.fit(X_train_set, Y_train[Lable])\r\n",
    "\r\n",
    "    if is_print:\r\n",
    "        print(pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending = False)[['param_max_depth', 'param_n_estimators', 'std_test_score', 'mean_test_score', 'rank_test_score']][0:5])\r\n",
    "\r\n",
    "    return(pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending = False)[['param_max_depth',\r\n",
    "     'param_n_estimators', 'std_test_score', 'mean_test_score', 'rank_test_score']][0:5])\r\n",
    "  \r\n",
    "param = {'n_estimators' : [50, 100, 150],\r\n",
    "                'max_depth' : [7, 11, 15],\r\n",
    "                'Learning_rate' : [0.1]}\r\n",
    "\r\n",
    "classes = ['IE' , 'NS', 'FT', 'PJ']\r\n",
    "Y_train = pd.read_pickle('Y_train.pkl')\r\n",
    "gb_parm_result = []\r\n",
    "\r\n",
    "\r\n",
    "for feature_type in ['TFIDF', 'CountVectorizer', 'N-gram']:\r\n",
    "    if feature_type == 'TFIDF':\r\n",
    "        X_train_set = np.load('X_train_tfidf.npy')\r\n",
    "    elif feature_type == 'CountVectorizer':\r\n",
    "        X_train_set = np.load('X_train_count.npy')\r\n",
    "    elif feature_type == 'N-gram':\r\n",
    "        #X_train_set = np.load('X_train_ngram.npy')\r\n",
    "        print()\r\n",
    "    for item in classes:\r\n",
    "        gb_parm_result.append(GB_Evaluation(param = param, Lable = item))\r\n",
    "\r\n",
    "pd.DataFrame(gb_parm_result).to_csv('GB_Parm_Result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "frequency = 2500  # Set Frequency To 2500 Hertz\n",
    "duration = 1000  # Set Duration To 1000 ms == 1 second\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "6bd3622f6f3f8bb6e8b4643a72dc4bf7a75a1467dea0f2ef918aabb1c737dfb1"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}